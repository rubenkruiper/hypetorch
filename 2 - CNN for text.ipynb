{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### CNN's for NLP\n",
    "\n",
    "\n",
    "* http://www.wildml.com/2015/11/understanding-convolutional-neural-networks-for-nlp/\n",
    "* dataset: https://github.com/spro/practical-pytorch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.autograd import Variable\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import text-data from txt.files\n",
    "__Dataset import__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['dataset/names/Arabic.txt', 'dataset/names/Chinese.txt', 'dataset/names/Czech.txt', 'dataset/names/Dutch.txt', 'dataset/names/English.txt', 'dataset/names/French.txt', 'dataset/names/German.txt', 'dataset/names/Greek.txt', 'dataset/names/Irish.txt', 'dataset/names/Italian.txt', 'dataset/names/Japanese.txt', 'dataset/names/Korean.txt', 'dataset/names/Polish.txt', 'dataset/names/Portuguese.txt', 'dataset/names/Russian.txt', 'dataset/names/Scottish.txt', 'dataset/names/Spanish.txt', 'dataset/names/Vietnamese.txt']\n"
     ]
    }
   ],
   "source": [
    "# dataset location: ./dataset/names/*.txt\n",
    "import glob\n",
    "\n",
    "all_filenames = glob.glob('dataset/names/*.txt')\n",
    "print(all_filenames)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Convert to non-ascii characters__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Slusarski\n"
     ]
    }
   ],
   "source": [
    "import unicodedata\n",
    "import string\n",
    "\n",
    "all_letters = string.ascii_letters + \"_- .,;'0123456789\"\n",
    "n_letters = len(all_letters)\n",
    "\n",
    "# Turn a Unicode string to plain ASCII, thanks to http://stackoverflow.com/a/518232/2809427\n",
    "def unicode_to_ascii(s):\n",
    "    return ''.join(\n",
    "        c for c in unicodedata.normalize('NFD', s)\n",
    "        if unicodedata.category(c) != 'Mn'\n",
    "        and c in all_letters\n",
    "    )\n",
    "\n",
    "print(unicode_to_ascii('Ślusàrski'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Determine categories and words inside each txt file__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "n_categories = 18\n",
      "['Arabic', 'Chinese', 'Czech', 'Dutch', 'English', 'French', 'German', 'Greek', 'Irish', 'Italian', 'Japanese', 'Korean', 'Polish', 'Portuguese', 'Russian', 'Scottish', 'Spanish', 'Vietnamese']\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Build the category_lines dictionary, a list of names per language\n",
    "category_lines = {}\n",
    "all_categories = []\n",
    "\n",
    "# Read a file and split into lines\n",
    "def readLines(filename):\n",
    "    lines = open(filename).read().strip().split('\\n')\n",
    "    return [unicode_to_ascii(line) for line in lines]\n",
    "\n",
    "# create a list of words for each category\n",
    "for filename in all_filenames:\n",
    "    category = filename.split('/')[-1].split('.')[0]\n",
    "    all_categories.append(category)\n",
    "    category_lines[category] = readLines(filename)\n",
    "\n",
    "n_categories = len(all_categories)\n",
    "print('n_categories =', n_categories)\n",
    "\n",
    "# all_categories contains the keys to iterate over the category_lines dict\n",
    "print(all_categories)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Creating Tensors__\n",
    " \n",
    "Usually add padding to the character-sequences to normalise length for the CNN input. I'll try to Avoid this by treating the words as sequences of bi-grams:\n",
    "\n",
    "e.g. bigram-tensor for the word 'every'\n",
    "    \n",
    "|height (4)|width (2)     |\n",
    "|------|---:|\n",
    "|   |'ev'|\n",
    "|   |'ve'|\n",
    "|   |'er'|\n",
    "|   |'ry'|"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "aa\n",
      "52\n"
     ]
    }
   ],
   "source": [
    "# index all possible bigrams\n",
    "possible_bigrams = []\n",
    "for letter_1 in all_letters:\n",
    "    for letter_2 in all_letters:\n",
    "        possible_bigrams.append(letter_1 + letter_2)\n",
    "# reversed index & convert possible bigrams to dict\n",
    "all_bigrams = {bigram: index for index, bigram in enumerate(possible_bigrams)}\n",
    "possible_bigrams = {index: bigram for index, bigram in enumerate(possible_bigrams)}\n",
    "    \n",
    "print(possible_bigrams[0])\n",
    "print(all_bigrams['a_'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# function that takes a list of characters and ouputs bi-gram tensors with the same label\n",
    "def word_to_bigrams(word):\n",
    "    bigrams = []\n",
    "    if len(word) < 2:\n",
    "        # words consisting of a single letter are padded with a space ' '\n",
    "        return [word + ' ']\n",
    "    else:\n",
    "        list_of_chars = list(word)\n",
    "        # n-1 bigrams in a word\n",
    "        for i in range(len(list_of_chars) - 1):\n",
    "            bigrams.append([list_of_chars[i] + list_of_chars[i + 1]])\n",
    "        return bigrams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['te'], ['es'], ['st'], ['t_'], ['_w'], ['wo'], ['or'], ['rd']]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "word_to_bigrams('test_word')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "num_batches = 1 \n",
    "\n",
    "# split word into bigrams, create a \n",
    "def word_to_tensor(word):\n",
    "    # e.g. for the word 'every' a 4 by 2 tensor\n",
    "    tensor = torch.zeros(2, num_batches, len(possible_bigrams))\n",
    "    # each tensor is a single vector with a 1 for every bigram appearing\n",
    "    # this has to change!\n",
    "    for bigram in word_to_bigrams(word):\n",
    "        tensor[0][0][all_bigrams[bigram[0]]] =1\n",
    "    return tensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\n",
       "( 0  ,.,.) = \n",
       "   0   0   0  ...    0   0   0\n",
       "\n",
       "( 1  ,.,.) = \n",
       "   0   0   0  ...    0   0   0\n",
       "[torch.FloatTensor of size 2x1x4761]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "word_to_tensor('test_word')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# the X input is a list of all tensors, representing all names\n",
    "x_input = []\n",
    "for category in all_categories:\n",
    "    for name in category_lines[category]:\n",
    "        x_input.append(word_to_tensor(name))\n",
    "\n",
    "# the Y labels are the categories, where arabic is 0 and vietnamese is 17\n",
    "y_input = []\n",
    "for idx, category in enumerate(all_categories):\n",
    "    for i in range(0, len(category_lines[category])):\n",
    "        y_input.append(idx)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = []\n",
    "\n",
    "for idx, _ in enumerate(x_input):\n",
    "    data.append([x_input[idx], y_input[idx]])\n",
    "    \n",
    "from random import shuffle\n",
    "shuffle(data)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Amount of labels</th>\n",
       "      <th>Size of some tensors</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>6</td>\n",
       "      <td>(2, 1, 4761)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>14</td>\n",
       "      <td>(2, 1, 4761)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>14</td>\n",
       "      <td>(2, 1, 4761)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>(2, 1, 4761)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>14</td>\n",
       "      <td>(2, 1, 4761)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>0</td>\n",
       "      <td>(2, 1, 4761)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>14</td>\n",
       "      <td>(2, 1, 4761)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>0</td>\n",
       "      <td>(2, 1, 4761)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>4</td>\n",
       "      <td>(2, 1, 4761)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>8</td>\n",
       "      <td>(2, 1, 4761)</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Amount of labels Size of some tensors\n",
       "0                 6         (2, 1, 4761)\n",
       "1                14         (2, 1, 4761)\n",
       "2                14         (2, 1, 4761)\n",
       "3                 3         (2, 1, 4761)\n",
       "4                14         (2, 1, 4761)\n",
       "5                 0         (2, 1, 4761)\n",
       "6                14         (2, 1, 4761)\n",
       "7                 0         (2, 1, 4761)\n",
       "8                 4         (2, 1, 4761)\n",
       "9                 8         (2, 1, 4761)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "dataframe = {\n",
    "    'Amount of labels' : [data[i][1] for i in range(10)],\n",
    "    'Size of some tensors': [tensor.size() for tensor in x_input[9000:9010]]\n",
    "}\n",
    "\n",
    "pd.DataFrame(dataframe)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### CNN model\n",
    "* 2 convolutional layers\n",
    "* 2 pooling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class CNN(nn.Module):\n",
    "    def __init__(self, input_ch, conv1_ch, output_ch, kernel_size, fc_dim, output_size):\n",
    "        super(CNN, self).__init__()\n",
    "        \n",
    "        # Convolution 1\n",
    "        self.cnn1 = nn.Conv2d(in_channels=input_ch, out_channels=conv1_ch, kernel_size=kernel_size, stride=1,padding=2)\n",
    "        self.activation1 = nn.ReLU()\n",
    "        \n",
    "        # Max pool 1\n",
    "        self.maxpool1 = nn.MaxPool2d(kernel_size=2)\n",
    "        \n",
    "        #Convolution2\n",
    "        self.cnn2 = nn.Conv2d(in_channels=conv1_ch, out_channels=output_ch, kernel_size=kernel_size, stride=1,padding=2)\n",
    "        self.activation2 = nn.ReLU()\n",
    "        \n",
    "        # Max pool 2\n",
    "        self.maxpool2 = nn.MaxPool2d(kernel_size=2)\n",
    "        \n",
    "        # Fully connected \n",
    "        self.fc = nn.Linear(76288, output_size)\n",
    "        \n",
    "        \n",
    "    def forward(self, x):\n",
    "        # 3D tensor to 4D for the conv layer:\n",
    "        x = x.unsqueeze(0)\n",
    "        x = self.cnn1(x)\n",
    "        x = self.activation1(x)\n",
    "        x = self.maxpool1(x)\n",
    "        x = self.cnn2(x)\n",
    "        x = self.activation2(x)\n",
    "        x = self.maxpool2(x)\n",
    "        # Resize\n",
    "        # - original size: [wordlength - 1, batch_size, possible_bigrams: 4761]\n",
    "        # - x.size\n",
    "        # - new output size: [wordlength - 1, batch_size, possible_bigrams: 4761]\n",
    "        x = x.view(x.size(0),-1)\n",
    "        x = self.fc(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Kernel size__\n",
    "* $O = \\frac{W-K+2P}{S}+1$\n",
    "  * $O$: output heigth/length\n",
    "  * $W$: input height/length\n",
    "  * $K$: kernel size\n",
    "  * $P$: padding\n",
    "    * $ P = \\frac{K-1}{2}$\n",
    "  * $S$: Stride\n",
    "* $O$ = len(word_to_bigrams)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# input_ch = 1\n",
    "# conv1_ch = 16\n",
    "# output_ch, = 32\n",
    "# kernel_size = 2 to 5\n",
    "# fc_dim = 1\n",
    "# output_size = 18 classes\n",
    "### non-sliding kernel_height = 4761 (possible_bigrams)\n",
    "### sliding could be e.g. 529 (possible_bigrams/9)\n",
    "model = CNN(2,16,32,2,1,18)\n",
    "if torch.cuda.is_available():\n",
    "    model.cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#define some parameters\n",
    "\n",
    "epochs = 10\n",
    "batch_size = 32\n",
    "learning_rate = 0.01\n",
    "momentum = 0.9\n",
    "\n",
    "\n",
    "#define loss and optimizer\n",
    "\n",
    "criterion = nn.CrossEntropyLoss() #cross entropy loss = log softmax + NLL loss\n",
    "optimizer = optim.Adam(model.parameters())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#for plotting\n",
    "\n",
    "plot_loss = []\n",
    "plot_correct = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"print(model.parameters())\\nprint(len(list(model.parameters())))\\nprint('Conv1 kernels:\\n',list(model.parameters())[0].size())\\nprint('Conv1 bias kernels:\\n',list(model.parameters())[1].size())\\nprint('Conv2 kernels (depth 16):\\n',list(model.parameters())[2].size())\\nprint('Conv2 bias kernels:\\n',list(model.parameters())[3].size())\\nprint('Fully connected layer:\\n',list(model.parameters())[4].size())\\nprint('Fully connected bias:\\n',list(model.parameters())[5].size())\""
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Considering a cuda model, otherwise remove .cpu() or write if/else \n",
    "\"\"\"print(model.parameters())\n",
    "print(len(list(model.parameters())))\n",
    "print('Conv1 kernels:\\n',list(model.parameters())[0].size())\n",
    "print('Conv1 bias kernels:\\n',list(model.parameters())[1].size())\n",
    "print('Conv2 kernels (depth 16):\\n',list(model.parameters())[2].size())\n",
    "print('Conv2 bias kernels:\\n',list(model.parameters())[3].size())\n",
    "print('Fully connected layer:\\n',list(model.parameters())[4].size())\n",
    "print('Fully connected bias:\\n',list(model.parameters())[5].size())\"\"\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def train(model, criterion, optimizer, x, y):\n",
    "    x = Variable(x, requires_grad=False)\n",
    "    y = Variable(y, requires_grad=False)\n",
    "    \n",
    "    # reset gradient\n",
    "    optimizer.zero_grad()\n",
    "\n",
    "    # forward pass\n",
    "    fx = model.forward(x)\n",
    "    \n",
    "    # get the loss\n",
    "    loss = criterion(fx, y)\n",
    "\n",
    "    # backward pass\n",
    "    loss.backward()\n",
    "\n",
    "    # update parameters\n",
    "    optimizer.step()\n",
    "\n",
    "    # return the actual loss data, not the Variable\n",
    "    return loss.data[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\n",
       " 1.4039e+14\n",
       " 9.4418e+13\n",
       " 0.0000e+00\n",
       " 0.0000e+00\n",
       " 0.0000e+00\n",
       " 0.0000e+00\n",
       "[torch.cuda.LongTensor of size 6 (GPU 0)]"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "torch.LongTensor(data[0][1]).cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[\n",
       " ( 0  ,.,.) = \n",
       "    0   0   0  ...    0   0   0\n",
       " \n",
       " ( 1  ,.,.) = \n",
       "    0   0   0  ...    0   0   0\n",
       " [torch.FloatTensor of size 2x1x4761], 6]"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 01, loss = 0.950206\n",
      "Epoch 02, loss = 0.637751\n",
      "Epoch 03, loss = 0.571735\n",
      "Epoch 04, loss = 0.537836\n",
      "Epoch 05, loss = 0.516724\n",
      "Epoch 06, loss = 0.503568\n",
      "Epoch 07, loss = 0.493467\n",
      "Epoch 08, loss = 0.481044\n",
      "Epoch 09, loss = 0.478606\n",
      "Epoch 10, loss = 0.470019\n"
     ]
    }
   ],
   "source": [
    "iter = 0 \n",
    "\n",
    "for e in range(1, epochs+1):\n",
    "    loss = 0.\n",
    "    for i, tensor in enumerate(data):\n",
    "\n",
    "        x = data[i][0].cuda() #converts list of indices to tensor of indices\n",
    "        y = torch.LongTensor([data[i][1]]).cuda()\n",
    "        \n",
    "        loss += train(model, criterion, optimizer, x, y)\n",
    "    plot_loss.append(loss/len(data))\n",
    "    print(\"Epoch %02d, loss = %f\" % (e, loss / len(data)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\n",
       " 14\n",
       "[torch.FloatTensor of size 1]"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.ticker as ticker\n",
    "%matplotlib inline\n",
    "\n",
    "plt.figure()\n",
    "plt.plot(all_losses)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Keep track of correct guesses in a confusion matrix\n",
    "confusion = torch.zeros(n_categories, n_categories)\n",
    "n_confusion = 10000\n",
    "\n",
    "# Just return an output given a line\n",
    "def evaluate(line_tensor):\n",
    "    return model(line_tensor)\n",
    "\n",
    "# Go through a bunch of examples and record which are correctly guessed\n",
    "for i in range(n_confusion):\n",
    "    category, line, category_tensor, line_tensor = random_training_pair()\n",
    "    output = evaluate(line_tensor)\n",
    "    guess, guess_i = category_from_output(output)\n",
    "    category_i = all_categories.index(category)\n",
    "    confusion[category_i][guess_i] += 1\n",
    "\n",
    "# Normalize by dividing every row by its sum\n",
    "for i in range(n_categories):\n",
    "    confusion[i] = confusion[i] / confusion[i].sum()\n",
    "\n",
    "# Set up plot\n",
    "fig = plt.figure()\n",
    "ax = fig.add_subplot(111)\n",
    "cax = ax.matshow(confusion.numpy())\n",
    "fig.colorbar(cax)\n",
    "\n",
    "# Set up axes\n",
    "ax.set_xticklabels([''] + all_categories, rotation=90)\n",
    "ax.set_yticklabels([''] + all_categories)\n",
    "\n",
    "# Force label at every tick\n",
    "ax.xaxis.set_major_locator(ticker.MultipleLocator(1))\n",
    "ax.yaxis.set_major_locator(ticker.MultipleLocator(1))\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Nguyen',\n",
       " 'Tron',\n",
       " 'Le',\n",
       " 'Pham',\n",
       " 'Huynh',\n",
       " 'Hoang',\n",
       " 'Phan',\n",
       " 'Vu',\n",
       " 'Vo',\n",
       " 'Dang',\n",
       " 'Bui',\n",
       " 'Do',\n",
       " 'Ho',\n",
       " 'Ngo',\n",
       " 'Duong',\n",
       " 'Ly',\n",
       " 'An',\n",
       " 'an',\n",
       " 'Bach',\n",
       " 'Banh',\n",
       " 'Cao',\n",
       " 'Chau',\n",
       " 'Chu',\n",
       " 'Chung',\n",
       " 'Chu',\n",
       " 'Diep',\n",
       " 'Doan',\n",
       " 'Dam',\n",
       " 'Dao',\n",
       " 'Dinh',\n",
       " 'Doan',\n",
       " 'Giang',\n",
       " 'Ha',\n",
       " 'Han',\n",
       " 'Kieu',\n",
       " 'Kim',\n",
       " 'La',\n",
       " 'Lac',\n",
       " 'Lam',\n",
       " 'Lieu',\n",
       " 'Luc',\n",
       " 'Luong',\n",
       " 'Luu',\n",
       " 'Ma',\n",
       " 'Mach',\n",
       " 'Mai',\n",
       " 'Nghiem',\n",
       " 'Phi',\n",
       " 'Pho',\n",
       " 'Phung',\n",
       " 'Quach',\n",
       " 'Quang',\n",
       " 'Quyen',\n",
       " 'Ta',\n",
       " 'Thach',\n",
       " 'Thai',\n",
       " 'Sai',\n",
       " 'Thi',\n",
       " 'Than',\n",
       " 'Thao',\n",
       " 'Thuy',\n",
       " 'Tieu',\n",
       " 'To',\n",
       " 'Ton',\n",
       " 'Tong',\n",
       " 'Trang',\n",
       " 'Trieu',\n",
       " 'Trinh',\n",
       " 'Truong',\n",
       " 'Van',\n",
       " 'Vinh',\n",
       " 'Vuong',\n",
       " 'Vuu']"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list_of_lines = [li for li in category_lines.values()]\n",
    "[line for line in list_of_lines[17]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['K',\n",
       " 'A',\n",
       " 'A',\n",
       " 'A',\n",
       " 'A',\n",
       " 'A',\n",
       " 'A',\n",
       " 'A',\n",
       " 'A',\n",
       " 'A',\n",
       " 'A',\n",
       " 'A',\n",
       " 'A',\n",
       " 'A',\n",
       " 'A',\n",
       " 'S',\n",
       " 'A',\n",
       " 'N']"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[line[0][:][0] for cat, line in enumerate(category_lines.values())]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1568"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "28*28*2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
