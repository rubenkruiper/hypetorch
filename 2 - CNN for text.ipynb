{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### CNN's for NLP\n",
    "\n",
    "\n",
    "* http://www.wildml.com/2015/11/understanding-convolutional-neural-networks-for-nlp/\n",
    "* dataset: https://github.com/spro/practical-pytorch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.autograd import Variable\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import text-data from txt.files\n",
    "__Dataset import__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['dataset/names/Arabic.txt', 'dataset/names/Chinese.txt', 'dataset/names/Czech.txt', 'dataset/names/Dutch.txt', 'dataset/names/English.txt', 'dataset/names/French.txt', 'dataset/names/German.txt', 'dataset/names/Greek.txt', 'dataset/names/Irish.txt', 'dataset/names/Italian.txt', 'dataset/names/Japanese.txt', 'dataset/names/Korean.txt', 'dataset/names/Polish.txt', 'dataset/names/Portuguese.txt', 'dataset/names/Russian.txt', 'dataset/names/Scottish.txt', 'dataset/names/Spanish.txt', 'dataset/names/Vietnamese.txt']\n"
     ]
    }
   ],
   "source": [
    "# dataset location: ./dataset/names/*.txt\n",
    "import glob\n",
    "\n",
    "all_filenames = glob.glob('dataset/names/*.txt')\n",
    "print(all_filenames)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Convert to non-ascii characters__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Slusarski\n"
     ]
    }
   ],
   "source": [
    "import unicodedata\n",
    "import string\n",
    "\n",
    "all_letters = string.ascii_letters + \"_- .,;'0123456789\"\n",
    "n_letters = len(all_letters)\n",
    "\n",
    "# Turn a Unicode string to plain ASCII, thanks to http://stackoverflow.com/a/518232/2809427\n",
    "def unicode_to_ascii(s):\n",
    "    return ''.join(\n",
    "        c for c in unicodedata.normalize('NFD', s)\n",
    "        if unicodedata.category(c) != 'Mn'\n",
    "        and c in all_letters\n",
    "    )\n",
    "\n",
    "print(unicode_to_ascii('Ślusàrski'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Determine categories and words inside each txt file__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "n_categories = 18\n",
      "['Arabic', 'Chinese', 'Czech', 'Dutch', 'English', 'French', 'German', 'Greek', 'Irish', 'Italian', 'Japanese', 'Korean', 'Polish', 'Portuguese', 'Russian', 'Scottish', 'Spanish', 'Vietnamese']\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Build the category_lines dictionary, a list of names per language\n",
    "category_lines = {}\n",
    "all_categories = []\n",
    "\n",
    "# Read a file and split into lines\n",
    "def readLines(filename):\n",
    "    lines = open(filename).read().strip().split('\\n')\n",
    "    return [unicode_to_ascii(line) for line in lines]\n",
    "\n",
    "# create a list of words for each category\n",
    "for filename in all_filenames:\n",
    "    category = filename.split('/')[-1].split('.')[0]\n",
    "    all_categories.append(category)\n",
    "    category_lines[category] = readLines(filename)\n",
    "\n",
    "n_categories = len(all_categories)\n",
    "print('n_categories =', n_categories)\n",
    "\n",
    "# all_categories contains the keys to iterate over the category_lines dict\n",
    "print(all_categories)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Creating Tensors__\n",
    " \n",
    "Usually add padding to the character-sequences to normalise length for the CNN input. Avoid this by treating each characters as a 1-hot vector of dimension n_letters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "num_batches = 1 \n",
    "\n",
    "# every letter in a word should be represented by a vector\n",
    "def word_to_tensor(word):\n",
    "    list_of_chars = list(word)\n",
    "    tensor = torch.zeros(1, num_batches, n_letters)\n",
    "    tensors = []\n",
    "    \n",
    "    # each tensor is a single vector with a 1 for every bigram appearing\n",
    "    for index, letter in enumerate(list_of_chars):\n",
    "        letter_index = all_letters.find(letter)\n",
    "        tensor[0][0][letter_index] = 1\n",
    "        tensors.append(tensor)\n",
    "    return tensors\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[\n",
       " (0 ,.,.) = \n",
       " \n",
       " Columns 0 to 18 \n",
       "     0   0   0   1   1   0   0   0   0   0   0   0   0   0   1   0   0   1   1\n",
       " \n",
       " Columns 19 to 37 \n",
       "     1   0   0   1   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
       " \n",
       " Columns 38 to 56 \n",
       "     0   0   0   0   0   0   0   0   0   0   0   0   0   0   1   0   0   0   0\n",
       " \n",
       " Columns 57 to 68 \n",
       "     0   0   0   0   0   0   0   0   0   0   0   0\n",
       " [torch.FloatTensor of size 1x1x69], \n",
       " (0 ,.,.) = \n",
       " \n",
       " Columns 0 to 18 \n",
       "     0   0   0   1   1   0   0   0   0   0   0   0   0   0   1   0   0   1   1\n",
       " \n",
       " Columns 19 to 37 \n",
       "     1   0   0   1   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
       " \n",
       " Columns 38 to 56 \n",
       "     0   0   0   0   0   0   0   0   0   0   0   0   0   0   1   0   0   0   0\n",
       " \n",
       " Columns 57 to 68 \n",
       "     0   0   0   0   0   0   0   0   0   0   0   0\n",
       " [torch.FloatTensor of size 1x1x69], \n",
       " (0 ,.,.) = \n",
       " \n",
       " Columns 0 to 18 \n",
       "     0   0   0   1   1   0   0   0   0   0   0   0   0   0   1   0   0   1   1\n",
       " \n",
       " Columns 19 to 37 \n",
       "     1   0   0   1   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
       " \n",
       " Columns 38 to 56 \n",
       "     0   0   0   0   0   0   0   0   0   0   0   0   0   0   1   0   0   0   0\n",
       " \n",
       " Columns 57 to 68 \n",
       "     0   0   0   0   0   0   0   0   0   0   0   0\n",
       " [torch.FloatTensor of size 1x1x69], \n",
       " (0 ,.,.) = \n",
       " \n",
       " Columns 0 to 18 \n",
       "     0   0   0   1   1   0   0   0   0   0   0   0   0   0   1   0   0   1   1\n",
       " \n",
       " Columns 19 to 37 \n",
       "     1   0   0   1   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
       " \n",
       " Columns 38 to 56 \n",
       "     0   0   0   0   0   0   0   0   0   0   0   0   0   0   1   0   0   0   0\n",
       " \n",
       " Columns 57 to 68 \n",
       "     0   0   0   0   0   0   0   0   0   0   0   0\n",
       " [torch.FloatTensor of size 1x1x69], \n",
       " (0 ,.,.) = \n",
       " \n",
       " Columns 0 to 18 \n",
       "     0   0   0   1   1   0   0   0   0   0   0   0   0   0   1   0   0   1   1\n",
       " \n",
       " Columns 19 to 37 \n",
       "     1   0   0   1   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
       " \n",
       " Columns 38 to 56 \n",
       "     0   0   0   0   0   0   0   0   0   0   0   0   0   0   1   0   0   0   0\n",
       " \n",
       " Columns 57 to 68 \n",
       "     0   0   0   0   0   0   0   0   0   0   0   0\n",
       " [torch.FloatTensor of size 1x1x69], \n",
       " (0 ,.,.) = \n",
       " \n",
       " Columns 0 to 18 \n",
       "     0   0   0   1   1   0   0   0   0   0   0   0   0   0   1   0   0   1   1\n",
       " \n",
       " Columns 19 to 37 \n",
       "     1   0   0   1   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
       " \n",
       " Columns 38 to 56 \n",
       "     0   0   0   0   0   0   0   0   0   0   0   0   0   0   1   0   0   0   0\n",
       " \n",
       " Columns 57 to 68 \n",
       "     0   0   0   0   0   0   0   0   0   0   0   0\n",
       " [torch.FloatTensor of size 1x1x69], \n",
       " (0 ,.,.) = \n",
       " \n",
       " Columns 0 to 18 \n",
       "     0   0   0   1   1   0   0   0   0   0   0   0   0   0   1   0   0   1   1\n",
       " \n",
       " Columns 19 to 37 \n",
       "     1   0   0   1   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
       " \n",
       " Columns 38 to 56 \n",
       "     0   0   0   0   0   0   0   0   0   0   0   0   0   0   1   0   0   0   0\n",
       " \n",
       " Columns 57 to 68 \n",
       "     0   0   0   0   0   0   0   0   0   0   0   0\n",
       " [torch.FloatTensor of size 1x1x69], \n",
       " (0 ,.,.) = \n",
       " \n",
       " Columns 0 to 18 \n",
       "     0   0   0   1   1   0   0   0   0   0   0   0   0   0   1   0   0   1   1\n",
       " \n",
       " Columns 19 to 37 \n",
       "     1   0   0   1   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
       " \n",
       " Columns 38 to 56 \n",
       "     0   0   0   0   0   0   0   0   0   0   0   0   0   0   1   0   0   0   0\n",
       " \n",
       " Columns 57 to 68 \n",
       "     0   0   0   0   0   0   0   0   0   0   0   0\n",
       " [torch.FloatTensor of size 1x1x69], \n",
       " (0 ,.,.) = \n",
       " \n",
       " Columns 0 to 18 \n",
       "     0   0   0   1   1   0   0   0   0   0   0   0   0   0   1   0   0   1   1\n",
       " \n",
       " Columns 19 to 37 \n",
       "     1   0   0   1   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
       " \n",
       " Columns 38 to 56 \n",
       "     0   0   0   0   0   0   0   0   0   0   0   0   0   0   1   0   0   0   0\n",
       " \n",
       " Columns 57 to 68 \n",
       "     0   0   0   0   0   0   0   0   0   0   0   0\n",
       " [torch.FloatTensor of size 1x1x69]]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "word_to_tensor('test_word')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# the X input is a list of all tensors, representing all names\n",
    "x_input = []\n",
    "for category in all_categories:\n",
    "    for name in category_lines[category]:\n",
    "        x_input.append(word_to_tensor(name))\n",
    "\n",
    "# the Y labels are the categories, where arabic is 0 and vietnamese is 17\n",
    "y_input = []\n",
    "for idx, category in enumerate(all_categories):\n",
    "    for i in range(0, len(category_lines[category])):\n",
    "        y_input.append(idx)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "data = []\n",
    "\n",
    "for idx, _ in enumerate(x_input):\n",
    "    data.append([x_input[idx], y_input[idx]])\n",
    "    \n",
    "from random import shuffle\n",
    "shuffle(data)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Label</th>\n",
       "      <th>Size of tensors</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>14</td>\n",
       "      <td>(1, 1, 69)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>4</td>\n",
       "      <td>(1, 1, 69)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>14</td>\n",
       "      <td>(1, 1, 69)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>16</td>\n",
       "      <td>(1, 1, 69)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>(1, 1, 69)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>2</td>\n",
       "      <td>(1, 1, 69)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>14</td>\n",
       "      <td>(1, 1, 69)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>14</td>\n",
       "      <td>(1, 1, 69)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>0</td>\n",
       "      <td>(1, 1, 69)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>4</td>\n",
       "      <td>(1, 1, 69)</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Label Size of tensors\n",
       "0     14      (1, 1, 69)\n",
       "1      4      (1, 1, 69)\n",
       "2     14      (1, 1, 69)\n",
       "3     16      (1, 1, 69)\n",
       "4      4      (1, 1, 69)\n",
       "5      2      (1, 1, 69)\n",
       "6     14      (1, 1, 69)\n",
       "7     14      (1, 1, 69)\n",
       "8      0      (1, 1, 69)\n",
       "9      4      (1, 1, 69)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "dataframe = {\n",
    "    'Label' : [data[i][1] for i in range(10)],\n",
    "    'Size of tensors': [tensor[0].size() for tensor in x_input[9000:9010]]\n",
    "}\n",
    "\n",
    "pd.DataFrame(dataframe)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### CNN model\n",
    "* 2 convolutional layers\n",
    "* 2 pooling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class CNN(nn.Module):\n",
    "    def __init__(self, input_ch, conv1_ch, output_ch, kernel_size, fc_dim, output_size):\n",
    "        super(CNN, self).__init__()\n",
    "        \n",
    "        # Convolution 1\n",
    "        self.cnn1 = nn.Conv2d(in_channels=input_ch, out_channels=conv1_ch, kernel_size=kernel_size, stride=1,padding=2)\n",
    "        self.activation1 = nn.ReLU()\n",
    "        \n",
    "        # Max pool 1\n",
    "        self.maxpool1 = nn.MaxPool2d(kernel_size=2)\n",
    "        \n",
    "        #Convolution2\n",
    "        self.cnn2 = nn.Conv2d(in_channels=conv1_ch, out_channels=output_ch, kernel_size=kernel_size, stride=1,padding=2)\n",
    "        self.activation2 = nn.ReLU()\n",
    "        \n",
    "        # Max pool 2\n",
    "        self.maxpool2 = nn.MaxPool2d(kernel_size=2)\n",
    "\n",
    "        # Fully connected \n",
    "        self.fc = nn.Linear(1216, output_size)\n",
    "        \n",
    "        \n",
    "    def forward(self, x):\n",
    "        # 3D tensor to 4D for the conv layer:\n",
    "        x = x.unsqueeze(0)\n",
    "        x = self.cnn1(x)\n",
    "        x = self.activation1(x)\n",
    "        x = self.maxpool1(x)\n",
    "        x = self.cnn2(x)\n",
    "        x = self.activation2(x)\n",
    "        x = self.maxpool2(x)\n",
    "        # Resize\n",
    "        # - original size: [wordlength - 1, batch_size, possible_bigrams: 69]\n",
    "        # - x.size\n",
    "        # - new output size: [wordlength - 1, batch_size, possible_bigrams: 69]\n",
    "        x = x.view(x.size(0),-1)\n",
    "        x = self.fc(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "68.0"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(n_letters - 2)/1 +1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Kernel size__\n",
    "* $O = \\frac{W-K+2P}{S}+1$\n",
    "  * $O$: output heigth/length\n",
    "  * $W$: input height/length\n",
    "  * $K$: kernel size\n",
    "  * $P$: padding\n",
    "    * $ P = \\frac{K-1}{2}$\n",
    "  * $S$: Stride\n",
    "* $O$ = len(word_to_bigrams)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# input_ch = 1\n",
    "# conv1_ch = 16\n",
    "# output_ch, = 32\n",
    "# kernel_size = 2 to 5\n",
    "# fc_dim = 1\n",
    "# output_size = 18 classes\n",
    "### non-sliding kernel_height = 4761 (possible_bigrams)\n",
    "### sliding could be e.g. 529 (possible_bigrams/9)\n",
    "model = CNN(1,16,32,2,1,18)\n",
    "if torch.cuda.is_available():\n",
    "    model.cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#define some parameters\n",
    "\n",
    "epochs = 10\n",
    "batch_size = 32\n",
    "learning_rate = 0.01\n",
    "momentum = 0.9\n",
    "\n",
    "\n",
    "#define loss and optimizer\n",
    "\n",
    "criterion = nn.CrossEntropyLoss() #cross entropy loss = log softmax + NLL loss\n",
    "optimizer = optim.Adam(model.parameters())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#for plotting\n",
    "\n",
    "plot_loss = []\n",
    "plot_correct = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"\\nprint(model.parameters())\\nprint(len(list(model.parameters())))\\nprint('Conv1 kernels:\\n',list(model.parameters())[0].size())\\nprint('Conv1 bias kernels:\\n',list(model.parameters())[1].size())\\nprint('Conv2 kernels (depth 16):\\n',list(model.parameters())[2].size())\\nprint('Conv2 bias kernels:\\n',list(model.parameters())[3].size())\\nprint('Fully connected layer:\\n',list(model.parameters())[4].size())\\nprint('Fully connected bias:\\n',list(model.parameters())[5].size())\""
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Considering a cuda model, otherwise remove .cpu() or write if/else \n",
    "'''\n",
    "print(model.parameters())\n",
    "print(len(list(model.parameters())))\n",
    "print('Conv1 kernels:\\n',list(model.parameters())[0].size())\n",
    "print('Conv1 bias kernels:\\n',list(model.parameters())[1].size())\n",
    "print('Conv2 kernels (depth 16):\\n',list(model.parameters())[2].size())\n",
    "print('Conv2 bias kernels:\\n',list(model.parameters())[3].size())\n",
    "print('Fully connected layer:\\n',list(model.parameters())[4].size())\n",
    "print('Fully connected bias:\\n',list(model.parameters())[5].size())'''\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def train(model, criterion, optimizer, x, y):\n",
    "    x = Variable(x, requires_grad=False)\n",
    "    y = Variable(y, requires_grad=False)\n",
    "    \n",
    "    # reset gradient\n",
    "    optimizer.zero_grad()\n",
    "\n",
    "    # forward pass\n",
    "    fx = model.forward(x)\n",
    "    \n",
    "    # get the loss\n",
    "    loss = criterion(fx, y)\n",
    "\n",
    "    # backward pass\n",
    "    loss.backward()\n",
    "\n",
    "    # update parameters\n",
    "    optimizer.step()\n",
    "\n",
    "    # return the actual loss data, not the Variable\n",
    "    return loss.data[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 01, loss = 1.262164\n",
      "Epoch 02, loss = 1.100251\n",
      "Epoch 03, loss = 1.071740\n",
      "Epoch 04, loss = 1.054534\n",
      "Epoch 05, loss = 1.042647\n",
      "Epoch 06, loss = 1.034538\n",
      "Epoch 07, loss = 1.028377\n",
      "Epoch 08, loss = 1.023641\n",
      "Epoch 09, loss = 1.019815\n",
      "Epoch 10, loss = 1.017065\n"
     ]
    }
   ],
   "source": [
    "iter = 0 \n",
    "\n",
    "for e in range(1, epochs+1):\n",
    "    loss = 0.\n",
    "    for i, tensor in enumerate(data):\n",
    "\n",
    "        x = data[i][0][0].cuda() #converts list of indices to tensor of indices\n",
    "        y = torch.LongTensor([data[i][1]]).cuda()\n",
    "        \n",
    "        loss += train(model, criterion, optimizer, x, y)\n",
    "    plot_loss.append(loss/len(data))\n",
    "    print(\"Epoch %02d, loss = %f\" % (e, loss / len(data)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<matplotlib.lines.Line2D at 0x7f6c4c66a2b0>]"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAX4AAAD8CAYAAABw1c+bAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7f6c4c6d5048>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.ticker as ticker\n",
    "%matplotlib inline\n",
    "\n",
    "plt.figure()\n",
    "plt.plot(plot_loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Keep track of correct guesses in a confusion matrix\n",
    "confusion = torch.zeros(n_categories, n_categories)\n",
    "n_confusion = 10000\n",
    "\n",
    "# Just return an output given a line\n",
    "def evaluate(line_tensor):\n",
    "    return model(line_tensor)\n",
    "\n",
    "# Go through a bunch of examples and record which are correctly guessed\n",
    "for i in range(n_confusion):\n",
    "    category, line, category_tensor, line_tensor = random_training_pair()\n",
    "    output = evaluate(line_tensor)\n",
    "    guess, guess_i = category_from_output(output)\n",
    "    category_i = all_categories.index(category)\n",
    "    confusion[category_i][guess_i] += 1\n",
    "\n",
    "# Normalize by dividing every row by its sum\n",
    "for i in range(n_categories):\n",
    "    confusion[i] = confusion[i] / confusion[i].sum()\n",
    "\n",
    "# Set up plot\n",
    "fig = plt.figure()\n",
    "ax = fig.add_subplot(111)\n",
    "cax = ax.matshow(confusion.numpy())\n",
    "fig.colorbar(cax)\n",
    "\n",
    "# Set up axes\n",
    "ax.set_xticklabels([''] + all_categories, rotation=90)\n",
    "ax.set_yticklabels([''] + all_categories)\n",
    "\n",
    "# Force label at every tick\n",
    "ax.xaxis.set_major_locator(ticker.MultipleLocator(1))\n",
    "ax.yaxis.set_major_locator(ticker.MultipleLocator(1))\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "list_of_lines = [li for li in category_lines.values()]\n",
    "[line for line in list_of_lines[17]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "[line[0][:][0] for cat, line in enumerate(category_lines.values())]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "28*28*2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
