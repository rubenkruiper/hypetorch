{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Basic steps of setting up a pytorch model\n",
    "1. __Data:__ preprocessing is outside the scope of this tutorial, the input is expect to be some torch tensor type\n",
    "2. __Network:__ input parameters, target parameters\n",
    "3. __Loss function__\n",
    "4. __Optimiser__\n",
    "5. __Training function__\n",
    "6. __Prediction function:__ use the trained network to make predictions on unseen input\n",
    "7. __Train the network__ using the given loss function and optimiser\n",
    "8. __Plot__ the losses, accuracy, etc..\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 1 - Data\n",
    "Working with torch tensors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Random tensor as seed 0: \n",
      " 0.5488  0.5928\n",
      " 0.7152  0.8443\n",
      "[torch.FloatTensor of size 2x2]\n",
      "\n",
      "Different random tensor without seed 0: \n",
      " 0.6028  0.8579\n",
      " 0.5449  0.8473\n",
      "[torch.FloatTensor of size 2x2]\n",
      "\n",
      "Same tensor as the first one, because we use seed 0: \n",
      " 0.5488  0.5928\n",
      " 0.7152  0.8443\n",
      "[torch.FloatTensor of size 2x2]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "# every seed produces the same random array\n",
    "torch.manual_seed(0)\n",
    "tensor_1 = torch.rand(2,2)\n",
    "\n",
    "tensor_2 = torch.rand(2,2)\n",
    "\n",
    "torch.manual_seed(0)\n",
    "tensor_3 = torch.rand(2,2)\n",
    "\n",
    "print('Random tensor as seed 0:', tensor_1)\n",
    "print('Different random tensor without seed 0:',tensor_2)\n",
    "print('Same tensor as the first one, because we use seed 0:',tensor_3)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Random tensor as seed 0: \n",
      " 0.6028  0.8579\n",
      " 0.5449  0.8473\n",
      "[torch.cuda.FloatTensor of size 2x2 (GPU 0)]\n",
      "\n",
      "Random tensor as seed 0: \n",
      " 0.4237  0.6236\n",
      " 0.6459  0.3844\n",
      "[torch.cuda.FloatTensor of size 2x2 (GPU 0)]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "if torch.cuda.is_available():\n",
    "    torch.cuda.manual_seed_all(0)\n",
    "    tensor_4 = torch.rand(2,2).cuda()\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    torch.cuda.manual_seed_all(0)\n",
    "    tensor_5 = torch.rand(2,2).cuda()\n",
    "    \n",
    "print('Random tensor as seed 0:', tensor_4)\n",
    "print('Random tensor as seed 0:', tensor_5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'torch.cuda.FloatTensor'>\n",
      "<class 'torch.FloatTensor'>\n"
     ]
    }
   ],
   "source": [
    "# Tensors on CPU vs GPU\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    tensor_1 = tensor_1.cuda()  # convert to gpu tensor with .cuda()\n",
    "\n",
    "print(type(tensor_1))\n",
    "\n",
    "tensor_1 = tensor_1.cpu()  # back to CPU with .cpu()\n",
    "print(type(tensor_1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### (re)size\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([2, 2])\n",
      "\n",
      " 0.5488  0.5928\n",
      " 0.7152  0.8443\n",
      "[torch.FloatTensor of size 2x2]\n",
      "\n",
      "torch.Size([4])\n",
      "\n",
      " 0.5488\n",
      " 0.5928\n",
      " 0.7152\n",
      " 0.8443\n",
      "[torch.FloatTensor of size 4]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(tensor_1.size())\n",
    "print(tensor_1)\n",
    "print(tensor_1.view(4).size())\n",
    "print(tensor_1.view(4))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### inplace is faster due to memory efficiency"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5.25 µs ± 42.8 ns per loop (mean ± std. dev. of 7 runs, 100000 loops each)\n"
     ]
    }
   ],
   "source": [
    "%timeit tensor_4 - tensor_5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4.79 µs ± 72.8 ns per loop (mean ± std. dev. of 7 runs, 100000 loops each)\n"
     ]
    }
   ],
   "source": [
    "%timeit tensor_4.sub(tensor_5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3.3 µs ± 6.72 ns per loop (mean ± std. dev. of 7 runs, 100000 loops each)\n"
     ]
    }
   ],
   "source": [
    "%timeit tensor_4.sub_(tensor_5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### subtract"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([2, 2])\n",
      "<class 'torch.cuda.FloatTensor'>\n",
      "torch.Size([2, 2])\n"
     ]
    }
   ],
   "source": [
    "print(tensor_1.size()) \n",
    "print(type(tensor_4)) # size works the same on cuda tensors\n",
    "print(tensor_4.size())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "1.00000e+05 *\n",
      " -3.4531 -5.0686\n",
      " -5.2706 -3.1224\n",
      "[torch.cuda.FloatTensor of size 2x2 (GPU 0)]\n",
      "\n",
      "\n",
      " 0.9864  0.8904\n",
      " 1.6070  0.9010\n",
      "[torch.FloatTensor of size 2x2]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(tensor_4.add(tensor_1.cuda())) # to add two tensor, they have to be the same type\n",
    "print(tensor_1.add(torch.rand(2,2).float())) # both GPU/CPU and float/long/double"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "1.00000e+05 *\n",
      " -3.4531 -5.0686\n",
      " -5.2706 -3.1224\n",
      "[torch.cuda.FloatTensor of size 2x2 (GPU 0)]\n",
      "\n",
      "\n",
      "1.00000e+05 *\n",
      " -3.4531 -5.0686\n",
      " -5.2706 -3.1224\n",
      "[torch.cuda.FloatTensor of size 2x2 (GPU 0)]\n",
      "\n",
      "\n",
      "1.00000e+05 *\n",
      " -3.4531 -5.0686\n",
      " -5.2706 -3.1224\n",
      "[torch.cuda.FloatTensor of size 2x2 (GPU 0)]\n",
      "\n",
      "\n",
      "1.00000e+05 *\n",
      " -3.4531 -5.0686\n",
      " -5.2706 -3.1224\n",
      "[torch.cuda.FloatTensor of size 2x2 (GPU 0)]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(tensor_4.sub(tensor_5))\n",
    "print(tensor_4)                 # sub subtracts tensor_4 with tensor_5 == tensor_4 - tensor_5\n",
    "print(tensor_4.sub_(tensor_5))\n",
    "print(tensor_4)                 # sub_ replaces tensor_4 with tensor_5"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### multiply elementwise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor_4: \n",
      "1.00000e+05 *\n",
      " -3.4531 -5.0686\n",
      " -5.2706 -3.1224\n",
      "[torch.cuda.FloatTensor of size 2x2 (GPU 0)]\n",
      "\n",
      "tensor_5: \n",
      " 0.4237  0.6236\n",
      " 0.6459  0.3844\n",
      "[torch.cuda.FloatTensor of size 2x2 (GPU 0)]\n",
      "\n",
      "Multiply elementwise: \n",
      "1.00000e+05 *\n",
      " -1.4629 -3.1606\n",
      " -3.4042 -1.2002\n",
      "[torch.cuda.FloatTensor of size 2x2 (GPU 0)]\n",
      "\n",
      "Multiply elementwise: \n",
      "1.00000e+05 *\n",
      " -1.4629 -3.1606\n",
      " -3.4042 -1.2002\n",
      "[torch.cuda.FloatTensor of size 2x2 (GPU 0)]\n",
      "\n",
      "Multiply inplace: \n",
      "1.00000e+05 *\n",
      " -1.4629 -3.1606\n",
      " -3.4042 -1.2002\n",
      "[torch.cuda.FloatTensor of size 2x2 (GPU 0)]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print('tensor_4:',tensor_4)\n",
    "print('tensor_5:',tensor_5)\n",
    "# mul multiplies tensor_4 with tensor_5 == tensor_4 - tensor_5\n",
    "print('Multiply elementwise:',tensor_4.mul(tensor_5))    \n",
    "print('Multiply elementwise:',tensor_4 * tensor_5)\n",
    "    # mul_ without using additional memory \n",
    "print('Multiply inplace:',tensor_4.mul_(tensor_5))    \n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### division elementwise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor_4: \n",
      "1.00000e+05 *\n",
      " -1.4629 -3.1606\n",
      " -3.4042 -1.2002\n",
      "[torch.cuda.FloatTensor of size 2x2 (GPU 0)]\n",
      "\n",
      "tensor_5: \n",
      " 0.4237  0.6236\n",
      " 0.6459  0.3844\n",
      "[torch.cuda.FloatTensor of size 2x2 (GPU 0)]\n",
      "\n",
      "Divide elementwise: \n",
      "1.00000e+05 *\n",
      " -3.4531 -5.0686\n",
      " -5.2706 -3.1224\n",
      "[torch.cuda.FloatTensor of size 2x2 (GPU 0)]\n",
      "\n",
      "Divide elementwise: \n",
      "1.00000e+05 *\n",
      " -3.4531 -5.0686\n",
      " -5.2706 -3.1224\n",
      "[torch.cuda.FloatTensor of size 2x2 (GPU 0)]\n",
      "\n",
      "Divide inplace: \n",
      "1.00000e+05 *\n",
      " -3.4531 -5.0686\n",
      " -5.2706 -3.1224\n",
      "[torch.cuda.FloatTensor of size 2x2 (GPU 0)]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print('tensor_4:',tensor_4)\n",
    "print('tensor_5:',tensor_5)\n",
    "# mul multiplies tensor_4 with tensor_5 == tensor_4 - tensor_5\n",
    "print('Divide elementwise:',tensor_4.div(tensor_5))    \n",
    "print('Divide elementwise:',tensor_4 / tensor_5)\n",
    "    # mul_ without using additional memory \n",
    "print('Divide inplace:',tensor_4.div_(tensor_5))    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mean: \n",
      "1.00000e+05 *\n",
      " -4.3618\n",
      " -4.0955\n",
      "[torch.cuda.FloatTensor of size 2 (GPU 0)]\n",
      "\n",
      "std deviation: \n",
      "1.00000e+05 *\n",
      "  1.2852\n",
      "  1.3762\n",
      "[torch.cuda.FloatTensor of size 2 (GPU 0)]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# mean\n",
    "print('mean:', tensor_4.mean(dim=0))\n",
    "\n",
    "# standard deviation\n",
    "print('std deviation:', tensor_4.std(dim=0))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2 - Network\n",
    "2.1 Variables\n",
    "* A variable wraps a Tensor, this enables accumulation of gradients"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.autograd import Variable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Variable containing:\n",
       " 1  1\n",
       " 1  1\n",
       "[torch.FloatTensor of size 2x2]"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "var_a = Variable(torch.ones(2,2), requires_grad=True)\n",
    "var_a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Variable containing:\n",
      " 2  2\n",
      " 2  2\n",
      "[torch.FloatTensor of size 2x2]\n",
      "\n",
      "Variable containing:\n",
      " 1  1\n",
      " 1  1\n",
      "[torch.FloatTensor of size 2x2]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "var_b = Variable(torch.ones(2,2), requires_grad=True)\n",
    "print(var_a + var_b)\n",
    "# same add, sub, mul, div operations can be applied \n",
    "print(torch.mul(var_a, var_b))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "mul_ only supports scalar multiplication",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-17-bcc61c2ed5cc>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# But not in place! Why?\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;31m# Because \"mul_ only supports scalar multiplication\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvar_a\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmul_\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvar_b\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/scratch/anaconda3/envs/pytorch/lib/python3.6/site-packages/torch/autograd/variable.py\u001b[0m in \u001b[0;36mmul_\u001b[0;34m(self, other)\u001b[0m\n\u001b[1;32m    345\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mother\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mVariable\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mis_tensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mother\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    346\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mMulConstant\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mapply\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mother\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 347\u001b[0;31m         \u001b[0;32mraise\u001b[0m \u001b[0mRuntimeError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"mul_ only supports scalar multiplication\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    348\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    349\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mdiv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mother\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: mul_ only supports scalar multiplication"
     ]
    }
   ],
   "source": [
    "# But not in place! Why?\n",
    "# Because \"mul_ only supports scalar multiplication\"\n",
    "print(var_a.mul_(var_b))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### 2.2 Gradients\n",
    "\n",
    "requires_grad enables calculation of gradients for Variables\n",
    "* Define original equation\n",
    "* Substitute equation with x\n",
    "* Reduce to scalar output using mean > o\n",
    "* Calculate gradients > o.backward()\n",
    "* Access the gradients using x.grad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Variable containing:\n",
       " 1\n",
       " 1\n",
       "[torch.FloatTensor of size 2]"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x = Variable(torch.ones(2), requires_grad=True)\n",
    "x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Variable containing:\n",
       " 20\n",
       " 20\n",
       "[torch.FloatTensor of size 2]"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y = 5 * (x + 1) ** 2\n",
    "y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Variable containing:\n",
       " 20\n",
       "[torch.FloatTensor of size 1]"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "o = (1/2) * torch.sum(y)\n",
    "o"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "None\n"
     ]
    }
   ],
   "source": [
    "# calculate the gradients for variables involved in o\n",
    "print(o.backward())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Variable containing:\n",
       " 10\n",
       " 10\n",
       "[torch.FloatTensor of size 2]"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# the gradients are stored in the Variable\n",
    "x.grad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Variable containing:\n",
       " 10\n",
       " 10\n",
       "[torch.FloatTensor of size 2]"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "o.backward(torch.FloatTensor([1.0,1.0]))\n",
    "x.grad"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### 2.3 Logistic Regression with Pytorch\n",
    "* bring together some of the previously introduced ideas in a simple application\n",
    "\n",
    "y = ax + B\n",
    "g = Log(y)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAXYAAAD8CAYAAABjAo9vAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7f04912a7438>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "np.random.seed(1)\n",
    "n = 50\n",
    "x = np.random.randn(n)\n",
    "y = x * np.random.randn(n)\n",
    "\n",
    "colors = np.random.randn(n)\n",
    "plt.plot(np.unique(x), np.poly1d(np.polyfit(x, y, 1))(np.unique(x)))\n",
    "\n",
    "plt.scatter(x, y, c=colors, alpha=.5)\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(11,)"
      ]
     },
     "execution_count": 112,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_values = [i for i in range(11)]\n",
    "# convert to numpy array\n",
    "x_train = np.array(x_values, dtype=np.float32)\n",
    "x_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(11, 1)"
      ]
     },
     "execution_count": 113,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 2D array is required! therefore, reshape\n",
    "x_train = x_train.reshape(-1,1)\n",
    "x_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(11, 1)"
      ]
     },
     "execution_count": 114,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_values = [2*i + 1 for i in x_values]\n",
    "y_train = np.array(y_values, dtype=np.float32).reshape(-1,1)\n",
    "y_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.autograd import Variable"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### STEP 1 - CREATE THE MODEL CLASS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class Linreg(nn.Module):\n",
    "    def __init__(self, input_size, output_size):\n",
    "        super(Linreg, self).__init__()\n",
    "        # linear layer\n",
    "        self.linear = nn.Linear(input_dim, output_dim)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        out = self.linear(x)\n",
    "        return out"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### STEP 2 - INSTANTIATE THE MODEL CLASS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "input_dim = 1\n",
    "output_dim = 1\n",
    "\n",
    "model = Linreg(input_dim, output_dim)\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    model.cuda()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### STEP 3 - LOSS AND OPTIMIZER"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# loss function\n",
    "criterion = nn.MSELoss()\n",
    "# how are you updating the parameters for every epoch?\n",
    "learning_rate = 0.01\n",
    "optimizer = torch.optim.SGD(model.parameters(),lr = learning_rate)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### STEP 4 - TRAIN THE MODEL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 1, loss 325.5010\n",
      "epoch 2, loss 26.7578\n",
      "epoch 3, loss 2.3879\n",
      "epoch 4, loss 0.3979\n",
      "epoch 5, loss 0.2333\n",
      "epoch 6, loss 0.2176\n",
      "epoch 7, loss 0.2141\n",
      "epoch 8, loss 0.2116\n",
      "epoch 9, loss 0.2093\n",
      "epoch 10, loss 0.2069\n",
      "epoch 11, loss 0.2046\n",
      "epoch 12, loss 0.2023\n",
      "epoch 13, loss 0.2001\n",
      "epoch 14, loss 0.1978\n",
      "epoch 15, loss 0.1956\n",
      "epoch 16, loss 0.1935\n",
      "epoch 17, loss 0.1913\n",
      "epoch 18, loss 0.1892\n",
      "epoch 19, loss 0.1870\n",
      "epoch 20, loss 0.1850\n",
      "epoch 21, loss 0.1829\n",
      "epoch 22, loss 0.1808\n",
      "epoch 23, loss 0.1788\n",
      "epoch 24, loss 0.1768\n",
      "epoch 25, loss 0.1749\n",
      "epoch 26, loss 0.1729\n",
      "epoch 27, loss 0.1710\n",
      "epoch 28, loss 0.1691\n",
      "epoch 29, loss 0.1672\n",
      "epoch 30, loss 0.1653\n",
      "epoch 31, loss 0.1635\n",
      "epoch 32, loss 0.1616\n",
      "epoch 33, loss 0.1598\n",
      "epoch 34, loss 0.1580\n",
      "epoch 35, loss 0.1563\n",
      "epoch 36, loss 0.1545\n",
      "epoch 37, loss 0.1528\n",
      "epoch 38, loss 0.1511\n",
      "epoch 39, loss 0.1494\n",
      "epoch 40, loss 0.1477\n",
      "epoch 41, loss 0.1461\n",
      "epoch 42, loss 0.1445\n",
      "epoch 43, loss 0.1429\n",
      "epoch 44, loss 0.1413\n",
      "epoch 45, loss 0.1397\n",
      "epoch 46, loss 0.1381\n",
      "epoch 47, loss 0.1366\n",
      "epoch 48, loss 0.1351\n",
      "epoch 49, loss 0.1335\n",
      "epoch 50, loss 0.1321\n",
      "epoch 51, loss 0.1306\n",
      "epoch 52, loss 0.1291\n",
      "epoch 53, loss 0.1277\n",
      "epoch 54, loss 0.1263\n",
      "epoch 55, loss 0.1248\n",
      "epoch 56, loss 0.1234\n",
      "epoch 57, loss 0.1221\n",
      "epoch 58, loss 0.1207\n",
      "epoch 59, loss 0.1194\n",
      "epoch 60, loss 0.1180\n",
      "epoch 61, loss 0.1167\n",
      "epoch 62, loss 0.1154\n",
      "epoch 63, loss 0.1141\n",
      "epoch 64, loss 0.1128\n",
      "epoch 65, loss 0.1116\n",
      "epoch 66, loss 0.1103\n",
      "epoch 67, loss 0.1091\n",
      "epoch 68, loss 0.1079\n",
      "epoch 69, loss 0.1067\n",
      "epoch 70, loss 0.1055\n",
      "epoch 71, loss 0.1043\n",
      "epoch 72, loss 0.1031\n",
      "epoch 73, loss 0.1020\n",
      "epoch 74, loss 0.1009\n",
      "epoch 75, loss 0.0997\n",
      "epoch 76, loss 0.0986\n",
      "epoch 77, loss 0.0975\n",
      "epoch 78, loss 0.0964\n",
      "epoch 79, loss 0.0953\n",
      "epoch 80, loss 0.0943\n",
      "epoch 81, loss 0.0932\n",
      "epoch 82, loss 0.0922\n",
      "epoch 83, loss 0.0912\n",
      "epoch 84, loss 0.0901\n",
      "epoch 85, loss 0.0891\n",
      "epoch 86, loss 0.0881\n",
      "epoch 87, loss 0.0872\n",
      "epoch 88, loss 0.0862\n",
      "epoch 89, loss 0.0852\n",
      "epoch 90, loss 0.0843\n",
      "epoch 91, loss 0.0833\n",
      "epoch 92, loss 0.0824\n",
      "epoch 93, loss 0.0815\n",
      "epoch 94, loss 0.0806\n",
      "epoch 95, loss 0.0797\n",
      "epoch 96, loss 0.0788\n",
      "epoch 97, loss 0.0779\n",
      "epoch 98, loss 0.0770\n",
      "epoch 99, loss 0.0762\n",
      "epoch 100, loss 0.0753\n"
     ]
    }
   ],
   "source": [
    "epochs = 100\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    epoch += 1\n",
    "    # convert input data to torch Variable\n",
    "    if torch.cuda.is_available():\n",
    "        inputs = Variable(torch.from_numpy(x_train)).cuda()\n",
    "        labels = Variable(torch.from_numpy(y_train)).cuda()\n",
    "    else:\n",
    "        inputs = Variable(torch.from_numpy(x_train))\n",
    "        labels = Variable(torch.from_numpy(y_train))\n",
    "    \n",
    "    # Clear gradients w.r.t. parameters\n",
    "    optimizer.zero_grad()\n",
    "    \n",
    "    # Forward run to get outputs\n",
    "    outputs = model(inputs)\n",
    "    \n",
    "    # Calculate loss w.r.t labels\n",
    "    loss = criterion(outputs, labels)\n",
    "    \n",
    "    # Compute gradients w.r.t. parameters\n",
    "    loss.backward()\n",
    "    \n",
    "    # Update parameters\n",
    "    optimizer.step()\n",
    "    \n",
    "    print('epoch {}, loss {:.4f}'.format(epoch, loss.data[0]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### STEP 5 - PREDICT LABELS FOR TEST SET"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[  0.48948133],\n",
       "       [  2.56300068],\n",
       "       [  4.63651991],\n",
       "       [  6.71003914],\n",
       "       [  8.78355789],\n",
       "       [ 10.8570776 ],\n",
       "       [ 12.93059635],\n",
       "       [ 15.00411606],\n",
       "       [ 17.07763481],\n",
       "       [ 19.15115356],\n",
       "       [ 21.22467422]], dtype=float32)"
      ]
     },
     "execution_count": 129,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# seems like a lot of additional work to move back and forth from/to gpu\n",
    "if torch.cuda.is_available:\n",
    "    predicted = model(Variable(torch.from_numpy(x_train).cuda())).cpu().data.numpy()\n",
    "else:\n",
    "    predicted = model(Variable(torch.from_numpy(x_train))).data.numpy()\n",
    "predicted"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[  1.],\n",
       "       [  3.],\n",
       "       [  5.],\n",
       "       [  7.],\n",
       "       [  9.],\n",
       "       [ 11.],\n",
       "       [ 13.],\n",
       "       [ 15.],\n",
       "       [ 17.],\n",
       "       [ 19.],\n",
       "       [ 21.]], dtype=float32)"
      ]
     },
     "execution_count": 98,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAXQAAAD8CAYAAABn919SAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nkTihQBcRiRMKdBGROPH/AQ/TGEE2Y7chAAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7f048db0d898>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# clear plot\n",
    "plt.clf()\n",
    "\n",
    "# plot true data\n",
    "plt.plot(x_train, y_train, 'go', label='True data', alpha=.5)\n",
    "\n",
    "# plot predictions \n",
    "plt.plot(x_train, predicted, '--', label='Predictions', alpha=.5)\n",
    "\n",
    "# Legend and plot\n",
    "plt.legend(loc='best')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "metadata": {},
   "outputs": [],
   "source": [
    "save_model = True\n",
    "if save_model is True:\n",
    "    # save only parameters\n",
    "    torch.save(model.state_dict(), 'saved_shitty_test_model.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "metadata": {},
   "outputs": [],
   "source": [
    "load_model = True\n",
    "if load_model is True:\n",
    "    model.load_state_dict(torch.load('saved_shitty_test_model.pkl'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#parameters\n",
    "vocab_size = len(vocabulary)\n",
    "\n",
    "class Net(nn.Module):\n",
    "    def __init__(self, vocab_size, max_len, embedding_size, num_channels, output_size, dropout):\n",
    "        super(Net, self).__init__()\n",
    "        \n",
    "        # initialise a Network object containing e.g. the following layers  \n",
    "            # an embedding layer \n",
    "        self.embedding = nn.Embedding(vocab_size, embedding_size)\n",
    "            # a Linear hidden layer \n",
    "        self.hidden = nn.Linear(input_size, output_size)\n",
    "            # a convolutional layer taking input with 1 channel\n",
    "        self.conv = nn.Conv2d(in_channels=1, out_channels=num_channels, kernel_size=(3,embedding_size))\n",
    "            # an LSTM taking word embeddings as inputs, outputting hidden states\n",
    "        self.lstm = nn.LSTM(embedding_dimension, hidden_dimension)\n",
    "            # a quasi-recurrent NN input: 100, output: 200, dropout\n",
    "        self.qrnn = QRNN(100, 200, dropout=.5) \n",
    "        \n",
    "    \n",
    "    def init_hidden(self):\n",
    "        # Initialise hidden states for an lstm\n",
    "        # The axes semantics are (num_layers, minibatch_size, hidden_dim)\n",
    "        return (autograd.Variable(torch.zeros(1, 1, self.hidden_dim)),\n",
    "                autograd.Variable(torch.zeros(1, 1, self.hidden_dim)))\n",
    "    \n",
    "    \n",
    "    def forward(self, x):\n",
    "        # determine how the input is passed through the layers\n",
    "        # in example for input x, where x is a sentence        \n",
    "            # convert the vector of words to a vector of word embeddings\n",
    "        word_embeddings = self.embedding(x)\n",
    "            # run the word_embedding vector through an lstm\n",
    "        lstm_out, self.hidden = self.lstm(word_embeddings.view(len(x), 1, -1), self.hidden)\n",
    "            # activation function for the convolutional layer: relu\n",
    "            # the squeeze removes 1 argument / decrease the arity, \n",
    "            #      e.g. [batch_size,num_channels, Height, Width]\n",
    "            #      becomes [batch_size,num_channels, Height]\n",
    "        x_conv = F.relu(self.conv(x)).squeeze(3)\n",
    "            # \n",
    "        \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
