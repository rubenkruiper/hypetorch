{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### CNN's for NLP\n",
    "\n",
    "\n",
    "* http://www.wildml.com/2015/11/understanding-convolutional-neural-networks-for-nlp/\n",
    "* dataset: https://github.com/spro/practical-pytorch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.autograd import Variable\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import text-data from txt.files\n",
    "__Dataset import__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['dataset/names/Arabic.txt', 'dataset/names/Chinese.txt', 'dataset/names/Czech.txt', 'dataset/names/Dutch.txt', 'dataset/names/English.txt', 'dataset/names/French.txt', 'dataset/names/German.txt', 'dataset/names/Greek.txt', 'dataset/names/Irish.txt', 'dataset/names/Italian.txt', 'dataset/names/Japanese.txt', 'dataset/names/Korean.txt', 'dataset/names/Polish.txt', 'dataset/names/Portuguese.txt', 'dataset/names/Russian.txt', 'dataset/names/Scottish.txt', 'dataset/names/Spanish.txt', 'dataset/names/Vietnamese.txt']\n"
     ]
    }
   ],
   "source": [
    "# dataset location: ./dataset/names/*.txt\n",
    "import glob\n",
    "\n",
    "all_filenames = glob.glob('dataset/names/*.txt')\n",
    "print(all_filenames)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Convert to non-ascii characters__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Slusarski\n"
     ]
    }
   ],
   "source": [
    "import unicodedata\n",
    "import string\n",
    "\n",
    "all_letters = string.ascii_letters + \"_- .,;'0123456789\"\n",
    "n_letters = len(all_letters)\n",
    "\n",
    "# Turn a Unicode string to plain ASCII, thanks to http://stackoverflow.com/a/518232/2809427\n",
    "def unicode_to_ascii(s):\n",
    "    return ''.join(\n",
    "        c for c in unicodedata.normalize('NFD', s)\n",
    "        if unicodedata.category(c) != 'Mn'\n",
    "        and c in all_letters\n",
    "    )\n",
    "\n",
    "print(unicode_to_ascii('Ślusàrski'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Determine categories and words inside each txt file__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "n_categories = 18\n",
      "['Arabic', 'Chinese', 'Czech', 'Dutch', 'English', 'French', 'German', 'Greek', 'Irish', 'Italian', 'Japanese', 'Korean', 'Polish', 'Portuguese', 'Russian', 'Scottish', 'Spanish', 'Vietnamese']\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Build the category_lines dictionary, a list of names per language\n",
    "category_lines = {}\n",
    "all_categories = []\n",
    "\n",
    "# Read a file and split into lines\n",
    "def readLines(filename):\n",
    "    lines = open(filename).read().strip().split('\\n')\n",
    "    return [unicode_to_ascii(line) for line in lines]\n",
    "\n",
    "# create a list of words for each category\n",
    "for filename in all_filenames:\n",
    "    category = filename.split('/')[-1].split('.')[0]\n",
    "    all_categories.append(category)\n",
    "    category_lines[category] = readLines(filename)\n",
    "\n",
    "n_categories = len(all_categories)\n",
    "print('n_categories =', n_categories)\n",
    "\n",
    "# all_categories contains the keys to iterate over the category_lines dict\n",
    "print(all_categories)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Creating Tensors__\n",
    " \n",
    "Usually add padding to the character-sequences to normalise length for the CNN input. I'll try to Avoid this by treating the words as sequences of bi-grams:\n",
    "\n",
    "e.g. bigram-tensor for the word 'every'\n",
    "    \n",
    "|height (4)|width (2)     |\n",
    "|------|---:|\n",
    "|'e'|'v'|\n",
    "|'v'|'e'|\n",
    "|'e'|'r'|\n",
    "|'r'|'y'|"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "aa\n",
      "52\n"
     ]
    }
   ],
   "source": [
    "# index all possible bigrams\n",
    "possible_bigrams = []\n",
    "for letter_1 in all_letters:\n",
    "    for letter_2 in all_letters:\n",
    "        possible_bigrams.append(letter_1 + letter_2)\n",
    "# reversed index & convert possible bigrams to dict\n",
    "all_bigrams = {bigram: index for index, bigram in enumerate(possible_bigrams)}\n",
    "possible_bigrams = {index: bigram for index, bigram in enumerate(possible_bigrams)}\n",
    "    \n",
    "print(possible_bigrams[0])\n",
    "print(all_bigrams['a_'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# function that takes a list of characters and ouputs bi-gram tensors with the same label\n",
    "def word_to_bigrams(word):\n",
    "    bigrams = []\n",
    "    if len(word) < 2:\n",
    "        # words consisting of a single letter are padded with a space ' '\n",
    "        return [word + ' ']\n",
    "    else:\n",
    "        list_of_chars = list(word)\n",
    "        # n-1 bigrams in a word\n",
    "        for i in range(len(list_of_chars) - 1):\n",
    "            bigrams.append([list_of_chars[i] + list_of_chars[i + 1]])\n",
    "        return bigrams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 te\n",
      "1 es\n",
      "2 st\n",
      "3 t_\n",
      "4 _w\n",
      "5 wo\n",
      "6 or\n",
      "7 rd\n"
     ]
    }
   ],
   "source": [
    "for index, i in enumerate(word_to_bigrams('test_word')):\n",
    "    for letter in i:\n",
    "        print(index, letter)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "num_batches = 1 \n",
    "\n",
    "# every letter in a word should be represented by a vector\n",
    "def word_to_tensor(word):\n",
    "    list_of_chars = list(word)\n",
    "    tensor = torch.zeros(2, num_batches, n_letters)\n",
    "    tensors = []\n",
    "    \n",
    "    # each tensor is a single vector with a 1 for every bigram appearing\n",
    "    for index, letter in enumerate(list_of_chars):\n",
    "        letter_index = all_letters.find(letter)\n",
    "        tensor[0][0][letter_index] = 1\n",
    "        tensors.append(tensor)\n",
    "    return tensors\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.FloatTensor"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(word_to_tensor('test')[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 465,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "num_batches = 1 \n",
    "\n",
    "def letter_to_tensor(letter):\n",
    "    tensor = torch.zeros(1, n_letters)\n",
    "    letter_index = all_letters.find(letter)\n",
    "    tensor[0][letter_index] = 1\n",
    "    return tensor\n",
    "\n",
    "# create a tensor for each \n",
    "def word_to_tensor(word):\n",
    "    tensor = torch.zeros(2, num_batches, n_letters)\n",
    "    tensors = []\n",
    "    # each tensor is a single vector with a 1 for every bigram appearing\n",
    "    # this has to change!\n",
    "    for bigram in word_to_bigrams(word):\n",
    "        for index, letter in enumerate(bigram):\n",
    "            letter_index = all_letters.find(letter)\n",
    "            # mistake here\n",
    "            tensor[index][0][letter_index] = 1\n",
    "        tensors.append(tensor)\n",
    "    return tensors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 477,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'list'>\n",
      "torch.Size([2, 1, 69])\n"
     ]
    }
   ],
   "source": [
    "print(type(word_to_tensor('test_word')))\n",
    "print(word_to_tensor('test_word')[0].size())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 479,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[\n",
       " (0 ,.,.) = \n",
       " \n",
       " Columns 0 to 18 \n",
       "     0   1   0   0   1   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
       " \n",
       " Columns 19 to 37 \n",
       "     0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
       " \n",
       " Columns 38 to 56 \n",
       "     0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
       " \n",
       " Columns 57 to 68 \n",
       "     0   0   0   0   0   0   0   0   0   0   0   0\n",
       " \n",
       " (1 ,.,.) = \n",
       " \n",
       " Columns 0 to 18 \n",
       "     0   0   0   0   1   0   0   0   0   0   0   0   0   1   0   0   0   0   0\n",
       " \n",
       " Columns 19 to 37 \n",
       "     0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
       " \n",
       " Columns 38 to 56 \n",
       "     0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
       " \n",
       " Columns 57 to 68 \n",
       "     0   0   0   0   0   0   0   0   0   0   0   0\n",
       " [torch.FloatTensor of size 2x1x69], \n",
       " (0 ,.,.) = \n",
       " \n",
       " Columns 0 to 18 \n",
       "     0   1   0   0   1   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
       " \n",
       " Columns 19 to 37 \n",
       "     0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
       " \n",
       " Columns 38 to 56 \n",
       "     0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
       " \n",
       " Columns 57 to 68 \n",
       "     0   0   0   0   0   0   0   0   0   0   0   0\n",
       " \n",
       " (1 ,.,.) = \n",
       " \n",
       " Columns 0 to 18 \n",
       "     0   0   0   0   1   0   0   0   0   0   0   0   0   1   0   0   0   0   0\n",
       " \n",
       " Columns 19 to 37 \n",
       "     0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
       " \n",
       " Columns 38 to 56 \n",
       "     0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
       " \n",
       " Columns 57 to 68 \n",
       "     0   0   0   0   0   0   0   0   0   0   0   0\n",
       " [torch.FloatTensor of size 2x1x69]]"
      ]
     },
     "execution_count": 479,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "19"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 384,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# the X input is a list of all tensors, representing all names\n",
    "x_input = []\n",
    "for category in all_categories:\n",
    "    for name in category_lines[category]:\n",
    "        x_input.append(word_to_tensor(name))\n",
    "\n",
    "# the Y labels are the categories, where arabic is 0 and vietnamese is 17\n",
    "y_input = []\n",
    "for idx, category in enumerate(all_categories):\n",
    "    for i in range(0, len(category_lines[category])):\n",
    "        y_input.append(idx)\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Create a pytorch dataset from the data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from torch.utils.data import DataSet\n",
    "\n",
    "class wordclassification(DataSet):\n",
    "    def __init__(self, input_list, label_list, transform=None):\n",
    "        '''\n",
    "        Args:\n",
    "            input_list: list with input values\n",
    "            label_list: list with labels\n",
    "            transform: Optional transform that may be applied\n",
    "        '''\n",
    "        self.inputs = [x for x in input_list]\n",
    "        self.labels = [torch.LongTensor(y) for y in label_list]\n",
    "        self.transform = transform\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.inputs)\n",
    "    \n",
    "    def __getitem__(self,idx):\n",
    "        \n",
    "        sample = \n",
    "        return sample\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 396,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "data = []\n",
    "\n",
    "for idx, _ in enumerate(x_input):\n",
    "    data.append([x_input[idx], y_input[idx]])\n",
    "    \n",
    "from random import shuffle\n",
    "shuffle(data)\n",
    "\n",
    "from torch.utils.data import TensorDataset\n",
    "for x, y in data:\n",
    "    dataset = TensorDataset(x, torch.LongTensor(2,y))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 397,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Amount of labels</th>\n",
       "      <th>Size of some tensors</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>10</td>\n",
       "      <td>(2, 1, 4761)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>14</td>\n",
       "      <td>(2, 1, 4761)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>14</td>\n",
       "      <td>(2, 1, 4761)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>(2, 1, 4761)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>14</td>\n",
       "      <td>(2, 1, 4761)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>0</td>\n",
       "      <td>(2, 1, 4761)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>4</td>\n",
       "      <td>(2, 1, 4761)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>3</td>\n",
       "      <td>(2, 1, 4761)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>0</td>\n",
       "      <td>(2, 1, 4761)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>4</td>\n",
       "      <td>(2, 1, 4761)</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Amount of labels Size of some tensors\n",
       "0                10         (2, 1, 4761)\n",
       "1                14         (2, 1, 4761)\n",
       "2                14         (2, 1, 4761)\n",
       "3                 4         (2, 1, 4761)\n",
       "4                14         (2, 1, 4761)\n",
       "5                 0         (2, 1, 4761)\n",
       "6                 4         (2, 1, 4761)\n",
       "7                 3         (2, 1, 4761)\n",
       "8                 0         (2, 1, 4761)\n",
       "9                 4         (2, 1, 4761)"
      ]
     },
     "execution_count": 397,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "dataframe = {\n",
    "    'Amount of labels' : [data[i][1] for i in range(10)],\n",
    "    'Size of some tensors': [tensor.size() for tensor in x_input[9000:9010]]\n",
    "}\n",
    "\n",
    "pd.DataFrame(dataframe)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### CNN model\n",
    "* 2 convolutional layers\n",
    "* 2 pooling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 327,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class CNN(nn.Module):\n",
    "    def __init__(self, input_ch, conv1_ch, output_ch, kernel_size, fc_dim, output_size):\n",
    "        super(CNN, self).__init__()\n",
    "        \n",
    "        # Convolution 1\n",
    "        self.cnn1 = nn.Conv2d(in_channels=input_ch, out_channels=conv1_ch, kernel_size=kernel_size, stride=1,padding=2)\n",
    "        self.activation1 = nn.ReLU()\n",
    "        \n",
    "        # Max pool 1\n",
    "        self.maxpool1 = nn.MaxPool2d(kernel_size=2)\n",
    "        \n",
    "        #Convolution2\n",
    "        self.cnn2 = nn.Conv2d(in_channels=conv1_ch, out_channels=output_ch, kernel_size=kernel_size, stride=1,padding=2)\n",
    "        self.activation2 = nn.ReLU()\n",
    "        \n",
    "        # Max pool 1\n",
    "        self.maxpool1 = nn.MaxPool2d(kernel_size=2)\n",
    "        \n",
    "        # Fully connected \n",
    "        self.fc = nn.Linear(output_ch, output_size)\n",
    "        \n",
    "        \n",
    "    def forward(self, x):\n",
    "        # 3D tensor to 4D for the conv layer:\n",
    "        x = x.unsqueeze(0)\n",
    "        x = self.cnn1(x)\n",
    "        x = self.activation1(x)\n",
    "        x = self.maxpool1(x)\n",
    "        x = self.cnn2(x)\n",
    "        x = self.activation2(x)\n",
    "        x = self.maxpool2(x)\n",
    "        # Resize\n",
    "        # - original size: [wordlength - 1, batch_size, possible_bigrams: 4761]\n",
    "        print(x.size())\n",
    "        # - x.size\n",
    "        # - new output size: [wordlength - 1, batch_size, possible_bigrams: 4761]\n",
    "        x = x.view(x.size(0),-1)\n",
    "        x = self.fc(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Kernel size__\n",
    "* $O = \\frac{W-K+2P}{S}+1$\n",
    "  * $O$: output heigth/length\n",
    "  * $W$: input height/length\n",
    "  * $K$: kernel size\n",
    "  * $P$: padding\n",
    "    * $ P = \\frac{K-1}{2}$\n",
    "  * $S$: Stride\n",
    "* $O$ = len(word_to_bigrams)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 355,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# input_ch = 1\n",
    "# conv1_ch = 16\n",
    "# output_ch, = 32\n",
    "# kernel_size = 2 to 5\n",
    "# fc_dim = 1\n",
    "# output_size = 18 classes\n",
    "### non-sliding kernel_height = 4761 (possible_bigrams)\n",
    "### sliding could be e.g. 529 (possible_bigrams/9)\n",
    "model = CNN(2,16,32,2,1,18)\n",
    "if torch.cuda.is_available():\n",
    "    model.cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 356,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#define some parameters\n",
    "\n",
    "epochs = 10\n",
    "batch_size = 32\n",
    "learning_rate = 0.01\n",
    "momentum = 0.9\n",
    "\n",
    "\n",
    "#define loss and optimizer\n",
    "\n",
    "criterion = nn.CrossEntropyLoss() #cross entropy loss = log softmax + NLL loss\n",
    "optimizer = optim.SGD(model.parameters(), lr=learning_rate, momentum=momentum)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 357,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#for plotting\n",
    "\n",
    "plot_loss = []\n",
    "plot_correct = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 358,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<generator object Module.parameters at 0x7fa3a2fcbd00>\n",
      "6\n",
      "Conv1 kernels:\n",
      " torch.Size([16, 2, 2, 2])\n",
      "Conv1 bias kernels:\n",
      " torch.Size([16])\n",
      "Conv2 kernels (depth 16):\n",
      " torch.Size([32, 16, 2, 2])\n",
      "Conv2 bias kernels:\n",
      " torch.Size([32])\n",
      "Fully connected layer:\n",
      " torch.Size([18, 32])\n",
      "Fully connected bias:\n",
      " torch.Size([18])\n"
     ]
    }
   ],
   "source": [
    "# Considering a cuda model, otherwise remove .cpu() or write if/else \n",
    "print(model.cpu().parameters())\n",
    "print(len(list(model.cpu().parameters())))\n",
    "print('Conv1 kernels:\\n',list(model.cpu().parameters())[0].size())\n",
    "print('Conv1 bias kernels:\\n',list(model.cpu().parameters())[1].size())\n",
    "print('Conv2 kernels (depth 16):\\n',list(model.cpu().parameters())[2].size())\n",
    "print('Conv2 bias kernels:\\n',list(model.cpu().parameters())[3].size())\n",
    "print('Fully connected layer:\\n',list(model.cpu().parameters())[4].size())\n",
    "print('Fully connected bias:\\n',list(model.cpu().parameters())[5].size())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 359,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def train(model, criterion, optimizer, x, y):\n",
    "    x = Variable(x, requires_grad=False)\n",
    "    y = Variable(y, requires_grad=False)\n",
    "    \n",
    "    # reset gradient\n",
    "    optimizer.zero_grad()\n",
    "\n",
    "    # forward pass\n",
    "    fx = model.forward(x)\n",
    "    \n",
    "    # get the loss\n",
    "    loss = criterion(fx, y)\n",
    "\n",
    "    # backward pass\n",
    "    loss.backward()\n",
    "\n",
    "    # update parameters\n",
    "    optimizer.step()\n",
    "\n",
    "    # return the actual loss data, not the Variable\n",
    "    return loss.data[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 375,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\n",
       " 1.4034e+14\n",
       " 1.4034e+14\n",
       "[torch.cuda.LongTensor of size 2 (GPU 0)]"
      ]
     },
     "execution_count": 375,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "torch.LongTensor(data[0][1]).cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 401,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(\n",
       "     0     0     0  ...      0     0     0\n",
       " [torch.FloatTensor of size 1x4761], \n",
       "  9.4173e+13\n",
       "  9.4173e+13\n",
       "  0.0000e+00\n",
       " -4.2950e+09\n",
       " [torch.LongTensor of size 4])"
      ]
     },
     "execution_count": 401,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 400,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'tuple' object has no attribute 'cuda'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-400-8fbcb2b4fbc6>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      4\u001b[0m     \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 6\u001b[0;31m         \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdataset\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcuda\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m#converts list of indices to tensor of indices\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      7\u001b[0m         \u001b[0my\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mLongTensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcuda\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'tuple' object has no attribute 'cuda'"
     ]
    }
   ],
   "source": [
    "iter = 0 \n",
    "\n",
    "for e in range(1, epochs+1):\n",
    "    loss = 0.\n",
    "    for i in range(len(dataset)):\n",
    "        x = dataset[i].cuda() #converts list of indices to tensor of indices\n",
    "        y = torch.LongTensor(2, data[i][1]).cuda()\n",
    "\n",
    "        loss += train(model, criterion, optimizer, x, y)\n",
    "    plot_loss.append(loss/num_examples)\n",
    "    print(\"Epoch %02d, loss = %f\" % (e, loss / num_examples))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "list_of_lines = [li for li in category_lines.values()]\n",
    "[line for line in list_of_lines[17]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Khoury',\n",
       " 'Ang',\n",
       " 'Abl',\n",
       " 'Aalsburg',\n",
       " 'Abbas',\n",
       " 'Abel',\n",
       " 'Abbing',\n",
       " 'Adamidis',\n",
       " 'Adam',\n",
       " 'Abandonato',\n",
       " 'Abe',\n",
       " 'Ahn',\n",
       " 'Adamczak',\n",
       " 'Abreu',\n",
       " 'Ababko',\n",
       " 'Smith',\n",
       " 'Abana',\n",
       " 'Nguyen']"
      ]
     },
     "execution_count": 73,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[line[0][:][0] for cat, line in enumerate(category_lines.values())]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 202,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1568"
      ]
     },
     "execution_count": 202,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "28*28*2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
