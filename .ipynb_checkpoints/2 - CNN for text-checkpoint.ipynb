{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### CNN's for NLP\n",
    "\n",
    "\n",
    "* http://www.wildml.com/2015/11/understanding-convolutional-neural-networks-for-nlp/\n",
    "* dataset: https://github.com/spro/practical-pytorch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.autograd import Variable\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import text-data from txt.files\n",
    "__Dataset import__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['dataset/names/Arabic.txt', 'dataset/names/Chinese.txt', 'dataset/names/Czech.txt', 'dataset/names/Dutch.txt', 'dataset/names/English.txt', 'dataset/names/French.txt', 'dataset/names/German.txt', 'dataset/names/Greek.txt', 'dataset/names/Irish.txt', 'dataset/names/Italian.txt', 'dataset/names/Japanese.txt', 'dataset/names/Korean.txt', 'dataset/names/Polish.txt', 'dataset/names/Portuguese.txt', 'dataset/names/Russian.txt', 'dataset/names/Scottish.txt', 'dataset/names/Spanish.txt', 'dataset/names/Vietnamese.txt']\n"
     ]
    }
   ],
   "source": [
    "# dataset location: ./dataset/names/*.txt\n",
    "import glob\n",
    "\n",
    "all_filenames = glob.glob('dataset/names/*.txt')\n",
    "print(all_filenames)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Convert to non-ascii characters__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Slusarski\n"
     ]
    }
   ],
   "source": [
    "import unicodedata\n",
    "import string\n",
    "\n",
    "all_letters = string.ascii_letters + \"- .,;'0123456789\"\n",
    "n_letters = len(all_letters)\n",
    "\n",
    "# Turn a Unicode string to plain ASCII, thanks to http://stackoverflow.com/a/518232/2809427\n",
    "def unicode_to_ascii(s):\n",
    "    return ''.join(\n",
    "        c for c in unicodedata.normalize('NFD', s)\n",
    "        if unicodedata.category(c) != 'Mn'\n",
    "        and c in all_letters\n",
    "    )\n",
    "\n",
    "print(unicode_to_ascii('Ślusàrski'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Determine categories and words inside each txt file__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "n_categories = 18\n",
      "['Arabic', 'Chinese', 'Czech', 'Dutch', 'English', 'French', 'German', 'Greek', 'Irish', 'Italian', 'Japanese', 'Korean', 'Polish', 'Portuguese', 'Russian', 'Scottish', 'Spanish', 'Vietnamese']\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Build the category_lines dictionary, a list of names per language\n",
    "category_lines = {}\n",
    "all_categories = []\n",
    "\n",
    "# Read a file and split into lines\n",
    "def readLines(filename):\n",
    "    lines = open(filename).read().strip().split('\\n')\n",
    "    return [unicode_to_ascii(line) for line in lines]\n",
    "\n",
    "# create a list of words for each category\n",
    "for filename in all_filenames:\n",
    "    category = filename.split('/')[-1].split('.')[0]\n",
    "    all_categories.append(category)\n",
    "    category_lines[category] = readLines(filename)\n",
    "\n",
    "n_categories = len(all_categories)\n",
    "print('n_categories =', n_categories)\n",
    "\n",
    "# all_categories contains the keys to iterate over the category_lines dict\n",
    "print(all_categories)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Creating Tensors__\n",
    " \n",
    "Usually add padding to the character-sequences to normalise length for the CNN input. I'll try to Avoid this by treating the words as sequences of bi-grams:\n",
    "\n",
    "e.g. bigram-tensor for the word 'every'\n",
    "    \n",
    "|height (4)|width (2)     |\n",
    "|------|---:|\n",
    "|   |'ev'|\n",
    "|   |'ve'|\n",
    "|   |'er'|\n",
    "|   |'ry'|"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "aa\n",
      "0\n"
     ]
    }
   ],
   "source": [
    "# index all possible bigrams\n",
    "possible_bigrams = []\n",
    "for letter_1 in all_letters:\n",
    "    for letter_2 in all_letters:\n",
    "        possible_bigrams.append(letter_1 + letter_2)\n",
    "# reversed index & convert possible bigrams to dict\n",
    "all_bigrams = {bigram: index for index, bigram in enumerate(possible_bigrams)}\n",
    "possible_bigrams = {index: bigram for index, bigram in enumerate(possible_bigrams)}\n",
    "    \n",
    "print(possible_bigrams[0])\n",
    "print(all_bigrams['aa'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# function that takes a list of characters and ouputs bi-gram tensors with the same label\n",
    "def word_to_bigrams(word):\n",
    "    bigrams = []\n",
    "    if len(word) < 2:\n",
    "        # words consisting of a single letter are padded with a space ' '\n",
    "        return [word + ' ']\n",
    "    else:\n",
    "        list_of_chars = list(word)\n",
    "        # n-1 bigrams in a word\n",
    "        for i in range(len(list_of_chars) - 1):\n",
    "            bigrams.append([list_of_chars[i] + list_of_chars[i + 1]])\n",
    "        return bigrams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['te'], ['es'], ['st'], ['t_'], ['_w'], ['wo'], ['or'], ['rd']]"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "word_to_bigrams('test_word')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_batches = 1 \n",
    "\n",
    "def word_to_tensor(word):\n",
    "    # e.g. for the word 'every' a 4 by 2 tensor\n",
    "    tensor = torch.zeros(len(word) - 1, num_batches, len(possible_bigrams))\n",
    "    print(type(tensor)) ## hiermee verder! goed de tensor-grootte bekijken\n",
    "    for bigram_nr, bigram in enumerate(word_to_bigrams(word)):\n",
    "        tensor[bigram_nr][0][all_bigrams[bigram[0]]] =1\n",
    "    return tensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'torch.FloatTensor'>\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "torch.Size([3, 1, 4624])"
      ]
     },
     "execution_count": 142,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "word_to_tensor('test').size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.FloatTensor"
      ]
     },
     "execution_count": 140,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(tensor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Nguyen',\n",
       " 'Tron',\n",
       " 'Le',\n",
       " 'Pham',\n",
       " 'Huynh',\n",
       " 'Hoang',\n",
       " 'Phan',\n",
       " 'Vu',\n",
       " 'Vo',\n",
       " 'Dang',\n",
       " 'Bui',\n",
       " 'Do',\n",
       " 'Ho',\n",
       " 'Ngo',\n",
       " 'Duong',\n",
       " 'Ly',\n",
       " 'An',\n",
       " 'an',\n",
       " 'Bach',\n",
       " 'Banh',\n",
       " 'Cao',\n",
       " 'Chau',\n",
       " 'Chu',\n",
       " 'Chung',\n",
       " 'Chu',\n",
       " 'Diep',\n",
       " 'Doan',\n",
       " 'Dam',\n",
       " 'Dao',\n",
       " 'Dinh',\n",
       " 'Doan',\n",
       " 'Giang',\n",
       " 'Ha',\n",
       " 'Han',\n",
       " 'Kieu',\n",
       " 'Kim',\n",
       " 'La',\n",
       " 'Lac',\n",
       " 'Lam',\n",
       " 'Lieu',\n",
       " 'Luc',\n",
       " 'Luong',\n",
       " 'Luu',\n",
       " 'Ma',\n",
       " 'Mach',\n",
       " 'Mai',\n",
       " 'Nghiem',\n",
       " 'Phi',\n",
       " 'Pho',\n",
       " 'Phung',\n",
       " 'Quach',\n",
       " 'Quang',\n",
       " 'Quyen',\n",
       " 'Ta',\n",
       " 'Thach',\n",
       " 'Thai',\n",
       " 'Sai',\n",
       " 'Thi',\n",
       " 'Than',\n",
       " 'Thao',\n",
       " 'Thuy',\n",
       " 'Tieu',\n",
       " 'To',\n",
       " 'Ton',\n",
       " 'Tong',\n",
       " 'Trang',\n",
       " 'Trieu',\n",
       " 'Trinh',\n",
       " 'Truong',\n",
       " 'Van',\n",
       " 'Vinh',\n",
       " 'Vuong',\n",
       " 'Vuu']"
      ]
     },
     "execution_count": 98,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list_of_lines = [li for li in category_lines.values()]\n",
    "[line for line in list_of_lines[17]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Khoury',\n",
       " 'Ang',\n",
       " 'Abl',\n",
       " 'Aalsburg',\n",
       " 'Abbas',\n",
       " 'Abel',\n",
       " 'Abbing',\n",
       " 'Adamidis',\n",
       " 'Adam',\n",
       " 'Abandonato',\n",
       " 'Abe',\n",
       " 'Ahn',\n",
       " 'Adamczak',\n",
       " 'Abreu',\n",
       " 'Ababko',\n",
       " 'Smith',\n",
       " 'Abana',\n",
       " 'Nguyen']"
      ]
     },
     "execution_count": 73,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[line[0][:][:] for cat, line in enumerate(category_lines.values())]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "all_letters.find('a')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
