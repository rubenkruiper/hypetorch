{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### CNN's for NLP\n",
    "\n",
    "\n",
    "* http://www.wildml.com/2015/11/understanding-convolutional-neural-networks-for-nlp/\n",
    "* dataset: https://github.com/spro/practical-pytorch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.autograd import Variable\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import text-data from txt.files\n",
    "__Dataset import__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['dataset/names/Arabic.txt', 'dataset/names/Chinese.txt', 'dataset/names/Czech.txt', 'dataset/names/Dutch.txt', 'dataset/names/English.txt', 'dataset/names/French.txt', 'dataset/names/German.txt', 'dataset/names/Greek.txt', 'dataset/names/Irish.txt', 'dataset/names/Italian.txt', 'dataset/names/Japanese.txt', 'dataset/names/Korean.txt', 'dataset/names/Polish.txt', 'dataset/names/Portuguese.txt', 'dataset/names/Russian.txt', 'dataset/names/Scottish.txt', 'dataset/names/Spanish.txt', 'dataset/names/Vietnamese.txt']\n"
     ]
    }
   ],
   "source": [
    "# dataset location: ./dataset/names/*.txt\n",
    "import glob\n",
    "\n",
    "all_filenames = glob.glob('dataset/names/*.txt')\n",
    "print(all_filenames)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Convert to non-ascii characters__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Slusarski\n"
     ]
    }
   ],
   "source": [
    "import unicodedata\n",
    "import string\n",
    "\n",
    "all_letters = string.ascii_letters + \"_- .,;'0123456789\"\n",
    "n_letters = len(all_letters)\n",
    "\n",
    "# Turn a Unicode string to plain ASCII, thanks to http://stackoverflow.com/a/518232/2809427\n",
    "def unicode_to_ascii(s):\n",
    "    return ''.join(\n",
    "        c for c in unicodedata.normalize('NFD', s)\n",
    "        if unicodedata.category(c) != 'Mn'\n",
    "        and c in all_letters\n",
    "    )\n",
    "\n",
    "print(unicode_to_ascii('Ślusàrski'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'a': 0, 'b': 1, 'c': 2, 'd': 3, 'e': 4, 'f': 5, 'g': 6, 'h': 7, 'i': 8, 'j': 9, 'k': 10, 'l': 11, 'm': 12, 'n': 13, 'o': 14, 'p': 15, 'q': 16, 'r': 17, 's': 18, 't': 19, 'u': 20, 'v': 21, 'w': 22, 'x': 23, 'y': 24, 'z': 25, 'A': 26, 'B': 27, 'C': 28, 'D': 29, 'E': 30, 'F': 31, 'G': 32, 'H': 33, 'I': 34, 'J': 35, 'K': 36, 'L': 37, 'M': 38, 'N': 39, 'O': 40, 'P': 41, 'Q': 42, 'R': 43, 'S': 44, 'T': 45, 'U': 46, 'V': 47, 'W': 48, 'X': 49, 'Y': 50, 'Z': 51, '_': 52, '-': 53, ' ': 54, '.': 55, ',': 56, ';': 57, \"'\": 58, '0': 59, '1': 60, '2': 61, '3': 62, '4': 63, '5': 64, '6': 65, '7': 66, '8': 67, '9': 68}\n"
     ]
    }
   ],
   "source": [
    "char2idx = {ch:i for i, ch in enumerate(all_letters)}\n",
    "idx2char = {i:ch for i, ch in enumerate(all_letters)}\n",
    "print(char2idx)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Determine categories and words inside each txt file__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "n_categories = 18\n",
      "['Arabic', 'Chinese', 'Czech', 'Dutch', 'English', 'French', 'German', 'Greek', 'Irish', 'Italian', 'Japanese', 'Korean', 'Polish', 'Portuguese', 'Russian', 'Scottish', 'Spanish', 'Vietnamese']\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Build the category_lines dictionary, a list of names per language\n",
    "category_lines = {}\n",
    "all_categories = []\n",
    "\n",
    "# Read a file and split into lines\n",
    "def readLines(filename):\n",
    "    lines = open(filename).read().strip().split('\\n')\n",
    "    return [unicode_to_ascii(line) for line in lines]\n",
    "\n",
    "# create a list of words for each category\n",
    "for filename in all_filenames:\n",
    "    category = filename.split('/')[-1].split('.')[0]\n",
    "    all_categories.append(category)\n",
    "    category_lines[category] = readLines(filename)\n",
    "\n",
    "n_categories = len(all_categories)\n",
    "print('n_categories =', n_categories)\n",
    "\n",
    "# all_categories contains the keys to iterate over the category_lines dict\n",
    "print(all_categories)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Creating Tensors__\n",
    " \n",
    "Usually add padding to the character-sequences to normalise length for the CNN input. Avoid this by treating each characters as a 1-hot vector of dimension n_letters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "num_batches = 1 \n",
    "\n",
    "# every letter in a word should be represented by a vector\n",
    "def word_to_tensor(word):\n",
    "    list_of_chars = list(word)\n",
    "    tensor = []\n",
    "    tensor.append([char2idx[char] for char in word])\n",
    "    return tensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[19, 4, 18, 19, 52, 22, 14, 17, 3]]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "word_to_tensor('test_word')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# the X input is a list of all tensors, representing all names\n",
    "x_input = []\n",
    "for category in all_categories:\n",
    "    for name in category_lines[category]:\n",
    "        x_input.append(word_to_tensor(name)[0])\n",
    "\n",
    "# the Y labels are the categories, where arabic is 0 and vietnamese is 17\n",
    "y_input = []\n",
    "for idx, category in enumerate(all_categories):\n",
    "    for i in range(0, len(category_lines[category])):\n",
    "        y_input.append(idx)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "data = []\n",
    "\n",
    "for idx, _ in enumerate(x_input):\n",
    "    data.append([x_input[idx][:], y_input[idx]])\n",
    "    \n",
    "from random import shuffle\n",
    "shuffle(data)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Amount of characters</th>\n",
       "      <th>Label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>6</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>5</td>\n",
       "      <td>14</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>5</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>6</td>\n",
       "      <td>14</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>6</td>\n",
       "      <td>14</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>7</td>\n",
       "      <td>14</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>6</td>\n",
       "      <td>11</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>6</td>\n",
       "      <td>14</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>8</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>4</td>\n",
       "      <td>14</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Amount of characters  Label\n",
       "0                     6      4\n",
       "1                     5     14\n",
       "2                     5      4\n",
       "3                     6     14\n",
       "4                     6     14\n",
       "5                     7     14\n",
       "6                     6     11\n",
       "7                     6     14\n",
       "8                     8      4\n",
       "9                     4     14"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "dataframe = {\n",
    "    'Label' : [data[i][1] for i in range(0,10)],\n",
    "    'Amount of characters': [len(tensor) for tensor in x_input[0:10]]\n",
    "}\n",
    "\n",
    "pd.DataFrame(dataframe)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### CNN model\n",
    "* 2 convolutional layers\n",
    "* 2 pooling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class CNN(nn.Module):\n",
    "    def __init__(self, input_size, output_size, embedding_size):\n",
    "        super(CNN, self).__init__()\n",
    "        \n",
    "        # Embedding\n",
    "        self.embedding = nn.Embedding(input_size, embedding_size)\n",
    "        \n",
    "        # Convolution 1\n",
    "        self.cnn1 = nn.Conv2d(in_channels=1, out_channels=1, kernel_size=(2,n_letters), stride=1)\n",
    "        self.activation1 = nn.ReLU()\n",
    "        self.maxpool1 = nn.MaxPool2d(kernel_size=(1,1))\n",
    "        \n",
    "        # Convolution2\n",
    "        self.cnn2 = nn.Conv2d(in_channels=1, out_channels=1, kernel_size=(5,n_letters), stride=1)\n",
    "        self.activation2 = nn.ReLU()\n",
    "        self.maxpool2 = nn.MaxPool2d(kernel_size=(1,1))\n",
    "        \n",
    "        # Convolution3\n",
    "        self.cnn3 = nn.Conv2d(in_channels=1, out_channels=1, kernel_size=(4,n_letters), stride=1)\n",
    "        self.activation3 = nn.ReLU()\n",
    "        self.maxpool3 = nn.MaxPool2d(kernel_size=(1,1))\n",
    "        \n",
    "        # Convolution4\n",
    "        self.cnn4 = nn.Conv1d(in_channels=1, out_channels=1, kernel_size=(5,n_letters), stride=1)\n",
    "        self.activation4 = nn.ReLU()\n",
    "        self.maxpool4 = nn.MaxPool2d(kernel_size=(1,1))\n",
    "        \n",
    "        # Convolution5\n",
    "        self.cnn5 = nn.Conv1d(in_channels=1, out_channels=1, kernel_size=(6,n_letters), stride=1)\n",
    "        self.activation5 = nn.ReLU()\n",
    "        self.maxpool5 = nn.MaxPool2d(kernel_size=(1,1))\n",
    "        \n",
    "        # Convolution6\n",
    "        self.cnn6 = nn.Conv1d(in_channels=1, out_channels=1, kernel_size=(7,n_letters), stride=1)\n",
    "        self.activation6 = nn.ReLU()\n",
    "        self.maxpool6 = nn.MaxPool2d(kernel_size=(1,1))\n",
    "        \n",
    "        # Convolution7\n",
    "        self.cnn7 = nn.Conv1d(in_channels=1, out_channels=1, kernel_size=(8,n_letters), stride=1)\n",
    "        self.activation7 = nn.ReLU()\n",
    "        self.maxpool7 = nn.MaxPool2d(kernel_size=(1,1))\n",
    "\n",
    "        # Fully connected \n",
    "        self.fc = nn.Linear(3584, output_size)\n",
    "        \n",
    "        \n",
    "    def forward(self, x):\n",
    "\n",
    "        print(x.size())\n",
    "        x = self.embedding(x)\n",
    "        print(x.size())\n",
    "        x = x.unsqueeze(0)\n",
    "        x = self.cnn1(x)\n",
    "        x = self.activation1(x)\n",
    "        x = self.maxpool1(x)\n",
    "        \n",
    "        print(x.size())\n",
    "        x = self.cnn2(x)\n",
    "        x = self.activation2(x)\n",
    "        x = self.maxpool2(x)\n",
    "        print(x.size())\n",
    "        x = self.cnn3(x)\n",
    "        x = self.activation3(x)\n",
    "        x = self.maxpool3(x)\n",
    "        print(x.size())\n",
    "        x = self.cnn4(x)\n",
    "        x = self.activation4(x)\n",
    "        x = self.maxpool4(x)\n",
    "        print(x.size())\n",
    "        x = self.cnn5(x)\n",
    "        x = self.activation5(x)\n",
    "        x = self.maxpool5(x)\n",
    "        print(x.size())\n",
    "        x = self.cnn6(x)\n",
    "        x = self.activation6(x)\n",
    "        x = self.maxpool6(x)\n",
    "        print(x.size())\n",
    "        x = self.cnn7(x)\n",
    "        x = self.activation7(x)\n",
    "        x = self.maxpool7(x)\n",
    "        print(x.size())\n",
    "        \n",
    "        # Resize\n",
    "        # - original size: [wordlength - 1, batch_size, possible_bigrams: 69]\n",
    "        # - x.size\n",
    "        # - new output size: [wordlength - 1, batch_size, possible_bigrams: 69]\n",
    "        # x = x.view(x.size(0),-1)\n",
    "        print(x.size())\n",
    "        x = self.fc(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "68.0"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(n_letters - 2)/1 +1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Kernel size__\n",
    "* $O = \\frac{W-K+2P}{S}+1$\n",
    "  * $O$: output heigth/length\n",
    "  * $W$: input height/length\n",
    "  * $K$: kernel size\n",
    "  * $P$: padding\n",
    "    * $ P = \\frac{K-1}{2}$\n",
    "  * $S$: Stride\n",
    "* $O$ = len(word_to_bigrams)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "\n",
    "# output_size = 18 classes\n",
    "# input_size = 69 different tokens\n",
    "# embedding_size = 69 different tokens?\n",
    "\n",
    "model = CNN(n_letters, 18, len(vocab))\n",
    "if torch.cuda.is_available():\n",
    "    model.cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#define some parameters\n",
    "\n",
    "epochs = 10\n",
    "batch_size = 32\n",
    "learning_rate = 0.01\n",
    "momentum = 0.9\n",
    "\n",
    "\n",
    "#define loss and optimizer\n",
    "\n",
    "criterion = nn.CrossEntropyLoss() #cross entropy loss = log softmax + NLL loss\n",
    "optimizer = optim.Adam(model.parameters())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#for plotting\n",
    "\n",
    "plot_loss = []\n",
    "plot_correct = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Considering a cuda model, otherwise remove .cpu() or write if/else \n",
    "'''\n",
    "print(model.parameters())\n",
    "print(len(list(model.parameters())))\n",
    "print('Conv1 kernels:\\n',list(model.parameters())[0].size())\n",
    "print('Conv1 bias kernels:\\n',list(model.parameters())[1].size())\n",
    "print('Conv2 kernels (depth 16):\\n',list(model.parameters())[2].size())\n",
    "print('Conv2 bias kernels:\\n',list(model.parameters())[3].size())\n",
    "print('Fully connected layer:\\n',list(model.parameters())[4].size())\n",
    "print('Fully connected bias:\\n',list(model.parameters())[5].size())'''\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def train(model, criterion, optimizer, x, y):\n",
    "    \n",
    "    # reset gradient\n",
    "    optimizer.zero_grad()\n",
    "\n",
    "    # forward pass\n",
    "    fx = model.forward(x)\n",
    "    \n",
    "    # get the loss\n",
    "    loss = criterion(fx, y)\n",
    "\n",
    "    # backward pass\n",
    "    loss.backward()\n",
    "\n",
    "    # update parameters\n",
    "    optimizer.step()\n",
    "\n",
    "    # return the actual loss data, not the Variable\n",
    "    return loss.data[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data[12][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "iter = 0 \n",
    "\n",
    "for e in range(1, epochs+1):\n",
    "    loss = 0.\n",
    "    for i, word in enumerate(data):\n",
    "        # HIER GAAT IETS MIS< ALLEEN EERSTE TENSOR WORDT NU MEEGENOMEN\n",
    "        if torch.cuda.is_available():\n",
    "            x = Variable(torch.LongTensor([data[i][0]]), requires_grad=False).cuda() #converts list of indices to tensor of indices\n",
    "            y = Variable(torch.LongTensor([data[i][1]]), requires_grad=False).cuda()\n",
    "        else:\n",
    "            x = Variable(torch.LongTensor([data[i][0]]), requires_grad=False)\n",
    "            y = Variable(torch.LongTensor([data[i][1]]), requires_grad=False)\n",
    "        \n",
    "        loss += train(model, criterion, optimizer, x, y)\n",
    "    plot_loss.append(loss/len(data))\n",
    "    print(\"Epoch %02d, loss = %f\" % (e, loss / len(data)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.ticker as ticker\n",
    "%matplotlib inline\n",
    "\n",
    "plt.figure()\n",
    "plt.plot(plot_loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "data[0][0]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import random\n",
    "def random_training_pair():                                                                                                          \n",
    "    category = random.choice(all_categories)\n",
    "    line = random.choice(category_lines[category])\n",
    "    category_tensor = Variable(torch.LongTensor([all_categories.index(category)]))\n",
    "    line_tensor = word_to_tensor(line)\n",
    "    return category, line, category_tensor, line_tensor\n",
    "\n",
    "\n",
    "def category_from_output(output):\n",
    "    top_n, top_i = output.data.topk(1) # Tensor out of Variable with .data\n",
    "    category_i = top_i[0][0]\n",
    "    return all_categories[category_i], category_i"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Keep track of correct guesses in a confusion matrix\n",
    "confusion = torch.zeros(n_categories, n_categories)\n",
    "n_confusion = 10000\n",
    "\n",
    "# Go through a bunch of examples and record which are correctly guessed\n",
    "for i in range(n_confusion):\n",
    "    category, line, category_tensor, line_tensor = random_training_pair()\n",
    "    output = model(line_tensor)\n",
    "    guess, guess_i = category_from_output(output)\n",
    "    category_i = all_categories.index(category)\n",
    "    confusion[category_i][guess_i] += 1\n",
    "\n",
    "# Normalize by dividing every row by its sum\n",
    "for i in range(n_categories):\n",
    "    confusion[i] = confusion[i] / confusion[i].sum()\n",
    "\n",
    "# Set up plot\n",
    "fig = plt.figure()\n",
    "ax = fig.add_subplot(111)\n",
    "cax = ax.matshow(confusion.numpy())\n",
    "fig.colorbar(cax)\n",
    "\n",
    "# Set up axes\n",
    "ax.set_xticklabels([''] + all_categories, rotation=90)\n",
    "ax.set_yticklabels([''] + all_categories)\n",
    "\n",
    "# Force label at every tick\n",
    "ax.xaxis.set_major_locator(ticker.MultipleLocator(1))\n",
    "ax.yaxis.set_major_locator(ticker.MultipleLocator(1))\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "list_of_lines = [li for li in category_lines.values()]\n",
    "[line for line in list_of_lines[17]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "[line[0][:][0] for cat, line in enumerate(category_lines.values())]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "28*28*2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
